{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "909cbb40-9b27-4a45-8acb-e229277cf219",
   "metadata": {},
   "source": [
    "## Comparison of CPU and GPU accelerated Genotype counting\n",
    "\n",
    "Firstly here is a CPU baseline for the computation of call_genotype statistics. This is a simplified version of the code in zarr_afdist.py.\n",
    "\n",
    "In this case, the data (int8) that is being loaded is comprised of 26504 variants across 2504 diploid samples.\n",
    "\n",
    "The data is loaded using Zarr using an Intel Xeon Silver 4216 x 64 CPU and Samsung NVMe drive (SSD 970 EVO Plus 2TB)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c27e694f-1bab-4626-8dec-680059ee1b31",
   "metadata": {
    "tags": []
   },
   "source": [
    "!vcf2zarr inspect chr22.zarr yields the following information about the chr22.zarr vcf archive:\n",
    "\n",
    "name                          dtype    stored      size          ratio    nchunks  chunk_size    avg_chunk_stored    shape              chunk_shape        compressor                                                      filters\n",
    "----------------------------  -------  ----------  ----------  -------  ---------  ------------  ------------------  -----------------  -----------------  --------------------------------------------------------------  ------------\n",
    "/call_PL                      int32    505.95 MiB  25.21 GiB     51            30  860.44 MiB    16.86 MiB           (96514, 2504, 28)  (10000, 1000, 28)  Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None\n",
    "/call_AD                      int16    199.34 MiB  3.15 GiB      16            30  107.56 MiB    6.64 MiB            (96514, 2504, 7)   (10000, 1000, 7)   Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None\n",
    "/call_DP                      int16    112.17 MiB  460.95 MiB     4.1          30  15.37 MiB     3.74 MiB            (96514, 2504)      (10000, 1000)      Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None\n",
    "/call_GQ                      int8     58.69 MiB   230.48 MiB     3.9          30  7.68 MiB      1.96 MiB            (96514, 2504)      (10000, 1000)      Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None\n",
    "/call_AB                      float32  31.02 MiB   921.9 MiB     30            30  30.73 MiB     1.03 MiB            (96514, 2504)      (10000, 1000)      Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None\n",
    "/call_PID                     object   9.36 MiB    1.8 GiB      200            30  61.46 MiB     319.48 KiB          (96514, 2504)      (10000, 1000)      Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   [VLenUTF8()]\n",
    "/call_genotype                int8     7.82 MiB    460.95 MiB    59            30  15.37 MiB     266.88 KiB          (96514, 2504, 2)   (10000, 1000, 2)   Blosc(cname='zstd', clevel=7, shuffle=BITSHUFFLE, blocksize=0)  None\n",
    "...   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49aa6833-3275-4249-948e-9023f0d0d097",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   start    stop  prob_dist\n",
      "0    0.0  0.1000    7081263\n",
      "1    0.1  0.2000    2212177\n",
      "2    0.2  0.3000    2096409\n",
      "3    0.3  0.4000    3027269\n",
      "4    0.4  0.5000   15854472\n",
      "5    0.5  0.6000    1769666\n",
      "6    0.6  0.7000     276639\n",
      "7    0.7  0.8000     223031\n",
      "8    0.8  0.9000     112195\n",
      "9    0.9  1.0125     488105\n",
      "Summarisation Time (ms) = 1457.252\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba\n",
    "import zarr\n",
    "import numcodecs\n",
    "import math\n",
    "import time\n",
    "\n",
    "dpath = \"/datasets/sgkit/vcf-zarr-publication/\"\n",
    "\n",
    "@numba.njit(\"void(int64, int8[:, :, :], int32[:], int32[:], int32[:], int32[:])\")\n",
    "def count_genotypes_chunk(offset, G, hom_ref, hom_alt, het, ref_count):\n",
    "    # NB Assuming diploids and no missing data!\n",
    "    index = offset\n",
    "    for j in range(G.shape[0]):\n",
    "        for k in range(G.shape[1]):\n",
    "            a = G[j, k, 0]\n",
    "            b = G[j, k, 1]\n",
    "            if a == b:\n",
    "                if a == 0:\n",
    "                    hom_ref[index] += 1\n",
    "                else:\n",
    "                    hom_alt[index] += 1\n",
    "            else:\n",
    "                het[index] += 1\n",
    "            ref_count[index] += (a == 0) + (b == 0)\n",
    "        index += 1\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class GenotypeCounts:\n",
    "    hom_ref: list\n",
    "    hom_alt: list\n",
    "    het: list\n",
    "    ref_count: list\n",
    "\n",
    "def classify_genotypes(call_genotype):\n",
    "    m = call_genotype.shape[0]\n",
    "\n",
    "    het = np.zeros(m, dtype=np.int32)\n",
    "    hom_alt = np.zeros(m, dtype=np.int32)\n",
    "    hom_ref = np.zeros(m, dtype=np.int32)\n",
    "    ref_count = np.zeros(m, dtype=np.int32)\n",
    "    j = 0\n",
    "    t1 = 0\n",
    "    \n",
    "    for v_chunk in range(call_genotype.cdata_shape[0]):\n",
    "        for s_chunk in range(call_genotype.cdata_shape[1]):\n",
    "            G = call_genotype.blocks[v_chunk, s_chunk]\n",
    "            t = time.time()\n",
    "            count_genotypes_chunk(j, G, hom_ref, hom_alt, het, ref_count)\n",
    "            t1 += (time.time()-t)\n",
    "        j += G.shape[0]\n",
    "   \n",
    "    return GenotypeCounts(hom_ref, hom_alt, het, ref_count), t1\n",
    "\n",
    "def zarr_afdist(root, num_bins=10, variant_slice=None, sample_slice=None):\n",
    "    call_genotype = root[\"call_genotype\"]\n",
    "    m = call_genotype.shape[0]\n",
    "    n = call_genotype.shape[1]\n",
    "\n",
    "    counts, t = classify_genotypes(call_genotype)\n",
    "\n",
    "    alt_count = 2 * n - counts.ref_count\n",
    "    af = alt_count / (n * 2)\n",
    "    bins = np.linspace(0, 1.0, num_bins + 1)\n",
    "    bins[-1] += 0.0125\n",
    "    pRA = 2 * af * (1 - af)\n",
    "    pAA = af * af\n",
    "\n",
    "    a = np.bincount(np.digitize(pRA, bins), weights=counts.het, minlength=num_bins + 1)\n",
    "    b = np.bincount(\n",
    "        np.digitize(pAA, bins), weights=counts.hom_alt, minlength=num_bins + 1\n",
    "    )\n",
    "\n",
    "    count = (a + b).astype(int)\n",
    "\n",
    "    return pd.DataFrame({\"start\": bins[:-1], \"stop\": bins[1:], \"prob_dist\": count[1:]}), t\n",
    "\n",
    "z_root = zarr.open(dpath + \"real_data/data/WGS/chr22.zarr\",mode='r')\n",
    "\n",
    "# warm up\n",
    "zarr_afdist(z_root)\n",
    "gc, t = zarr_afdist(z_root)\n",
    "\n",
    "print(gc)\n",
    "print(\"Summarisation Time (ms) = {:.3f}\".format(t*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a34a5-7d25-48bf-8563-c9805e23dc73",
   "metadata": {},
   "source": [
    "Note that what is measured in \"Summarisation Time\" is the time to compute the summarisation only (i.e. no loading the data)\n",
    "\n",
    "Next, the same computation is done using a cupy array and a cupy kernel. cupy arrays are functionally the same as numpy arrays, but reside within GPU memory.\n",
    "\n",
    "The GPU used is an Nvidia A6000 RTX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1258784-c0c5-459a-bdcc-31bd200abfd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/cupyx/jit/_interface.py:173: FutureWarning: cupyx.jit.rawkernel is experimental. The interface can change in the future.\n",
      "  cupy._util.experimental('cupyx.jit.rawkernel')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   start    stop  prob_dist\n",
      "0    0.0  0.1000    7081263\n",
      "1    0.1  0.2000    2212177\n",
      "2    0.2  0.3000    2096409\n",
      "3    0.3  0.4000    3027269\n",
      "4    0.4  0.5000   15854472\n",
      "5    0.5  0.6000    1769666\n",
      "6    0.6  0.7000     276639\n",
      "7    0.7  0.8000     223031\n",
      "8    0.8  0.9000     112195\n",
      "9    0.9  1.0125     488105\n",
      "Summarisation Time (ms) = 24.684\n",
      "Host to GPU Transfer Time (ms) = 179.458\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import time\n",
    "from cupyx import jit\n",
    "\n",
    "start_gpu = cp.cuda.Event()\n",
    "end_gpu = cp.cuda.Event()\n",
    "\n",
    "threads_per_block = 256\n",
    "\n",
    "@jit.rawkernel()\n",
    "def cu_count_genotypes_chunk(offset, G, hom_ref, hom_alt, het, ref_count, vs_size):\n",
    "    # NB Assuming diploids and no missing data!\n",
    "    idx = jit.blockIdx.x * jit.blockDim.x + jit.threadIdx.x\n",
    "    \n",
    "    variant_idx = idx + offset[0]\n",
    "\n",
    "    if idx<vs_size[0]:\n",
    "        for k in range(vs_size[1]):\n",
    "            a = G[idx, k, 0]\n",
    "            b = G[idx, k, 1]\n",
    "            if a == b:\n",
    "                if a == 0:\n",
    "                    hom_ref[variant_idx] += 1\n",
    "                else:\n",
    "                    hom_alt[variant_idx] += 1\n",
    "            else:\n",
    "                het[variant_idx] += 1\n",
    "\n",
    "            ref_count[variant_idx] += (a == 0)\n",
    "            ref_count[variant_idx] += (b == 0)\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class cu_GenotypeCounts:\n",
    "    hom_ref: 'np.ndarray[np.int32]'\n",
    "    hom_alt: 'np.ndarray[np.int32]'\n",
    "    het: 'np.ndarray[np.int32]'\n",
    "    ref_count: 'np.ndarray[np.int32]'\n",
    "    \n",
    "def cu_classify_genotypes_chunked(call_genotype):\n",
    "    m = call_genotype.shape[0]\n",
    "\n",
    "    cu_het = cp.zeros(m, dtype=np.int32)\n",
    "    cu_hom_alt = cp.zeros(m, dtype=np.int32)\n",
    "    cu_hom_ref = cp.zeros(m, dtype=np.int32)\n",
    "    cu_ref_count = cp.zeros(m, dtype=np.int32)\n",
    "    t_compute=0\n",
    "    t_transfer=0\n",
    "    \n",
    "    j = cp.zeros(1,dtype=cp.uint32)\n",
    "    for v_chunk in range(call_genotype.cdata_shape[0]):\n",
    "        for s_chunk in range(call_genotype.cdata_shape[1]):\n",
    "            # measure transfer time\n",
    "            G = call_genotype.blocks[v_chunk, s_chunk]\n",
    "            t = time.time()\n",
    "            cu_G = cp.array(G)\n",
    "            t_transfer += (time.time()-t)\n",
    "            vs_size = cp.zeros(2,cp.uint32)\n",
    "            vs_size[0] = cu_G.shape[0]\n",
    "            vs_size[1] = cu_G.shape[1]\n",
    "            start_gpu.record()\n",
    "            cu_count_genotypes_chunk[math.ceil(vs_size[0]/threads_per_block),threads_per_block](j, cu_G, cu_hom_ref, cu_hom_alt, cu_het, cu_ref_count,vs_size)\n",
    "            end_gpu.record()\n",
    "            end_gpu.synchronize()\n",
    "            t_compute += cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
    "        j += cu_G.shape[0]\n",
    "        \n",
    "    return cu_GenotypeCounts(cu_hom_ref, cu_hom_alt, cu_het, cu_ref_count), t_compute, t_transfer\n",
    "\n",
    "def cu_zarr_afdist(root, num_bins=10, variant_slice=None, sample_slice=None):\n",
    "    call_genotype = root[\"call_genotype\"]\n",
    "\n",
    "    n = call_genotype.shape[1]\n",
    "    counts, t1, t2 = cu_classify_genotypes_chunked(call_genotype)\n",
    "    alt_count = 2 * n - counts.ref_count\n",
    "    af = alt_count / (n * 2)\n",
    "    bins = cp.linspace(0, 1.0, num_bins + 1)\n",
    "    bins[-1] += 0.0125\n",
    "    pRA = 2 * af * (1 - af)\n",
    "    pAA = af * af\n",
    "    x = cp.digitize(cp.array(pRA).astype(np.float32), cp.array(bins).astype(np.float32))\n",
    "    a = cp.bincount(x, weights=counts.het, minlength=num_bins + 1)\n",
    "    y = cp.digitize(cp.array(pAA).astype(np.float32), cp.array(bins).astype(np.float32))\n",
    "    b = cp.bincount(\n",
    "        y, weights=counts.hom_alt, minlength=num_bins + 1\n",
    "    )\n",
    "    count = (a + b).astype(int)\n",
    "\n",
    "    return pd.DataFrame({\"start\": bins[:-1].get(), \"stop\": bins[1:].get(), \"prob_dist\": count[1:].get()}), t1, t2\n",
    "\n",
    "\n",
    "z_root = zarr.open(dpath + \"real_data/data/WGS/chr22.zarr\",mode='r')\n",
    "\n",
    "# warm up\n",
    "cu_zarr_afdist(z_root)\n",
    "gc, t1, t2 = cu_zarr_afdist(z_root)\n",
    "\n",
    "print(gc)\n",
    "print(\"Summarisation Time (ms) = {:.3f}\".format(t1))\n",
    "print(\"Host to GPU Transfer Time (ms) = {:.3f}\".format(t2*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6437d682-9b68-476c-b32d-de07417ff108",
   "metadata": {},
   "source": [
    "This simple kernel produces a modest speed-up, but there is more that can be done to optimise the cupy kernel.\n",
    "\n",
    "Next, the data is passed into the kernel in one large chunk and shared memory is used to cache interim results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4d3116e-3c0d-43e6-a05e-e939a6578f09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/cupyx/jit/_interface.py:173: FutureWarning: cupyx.jit.rawkernel is experimental. The interface can change in the future.\n",
      "  cupy._util.experimental('cupyx.jit.rawkernel')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   start    stop  prob_dist\n",
      "0    0.0  0.1000    7081263\n",
      "1    0.1  0.2000    2212177\n",
      "2    0.2  0.3000    2096409\n",
      "3    0.3  0.4000    3027269\n",
      "4    0.4  0.5000   15854472\n",
      "5    0.5  0.6000    1769666\n",
      "6    0.6  0.7000     276639\n",
      "7    0.7  0.8000     223031\n",
      "8    0.8  0.9000     112195\n",
      "9    0.9  1.0125     488105\n",
      "Summarisation Time (ms) = 21.120\n",
      "Host to GPU Transfer Time (ms) = 135.895\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import time\n",
    "from cupyx import jit\n",
    "\n",
    "threads_per_block = 64\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class cu_GenotypeCounts:\n",
    "    hom_ref: 'np.ndarray[np.int32]'\n",
    "    hom_alt: 'np.ndarray[np.int32]'\n",
    "    het: 'np.ndarray[np.int32]'\n",
    "    ref_count: 'np.ndarray[np.int32]'\n",
    "\n",
    "\n",
    "@jit.rawkernel()\n",
    "def cu_count_genotypes_chunk(variant_offset, G, hom_ref, hom_alt, het, ref_count,v_size,s_size):\n",
    "    # NB Assuming diploids and no missing data!\n",
    "    thread_idx = jit.threadIdx.x\n",
    "    grid_idx = jit.blockIdx.x * jit.blockDim.x + thread_idx\n",
    "    sm_offset = thread_idx*4\n",
    "    \n",
    "    variant_idx = grid_idx + variant_offset[0]\n",
    "\n",
    "    if grid_idx<v_size:\n",
    "        for k in range(s_size):\n",
    "            a = G[grid_idx, k, 0]\n",
    "            b = G[grid_idx, k, 1]\n",
    "            \n",
    "            if a == b:\n",
    "                if a == 0:\n",
    "                    hom_ref[variant_idx] += 1\n",
    "                else:\n",
    "                    hom_alt[variant_idx] += 1\n",
    "            else:\n",
    "                het[variant_idx] += 1\n",
    "            ref_count[variant_idx] += (a == 0)\n",
    "            ref_count[variant_idx] += (b == 0)\n",
    "    \n",
    "                    \n",
    "def cu_classify_genotypes(call_genotype):\n",
    "    m = call_genotype.shape[0]\n",
    "\n",
    "    cu_het = cp.zeros(m, dtype=np.int32)\n",
    "    cu_hom_alt = cp.zeros(m, dtype=np.int32)\n",
    "    cu_hom_ref = cp.zeros(m, dtype=np.int32)\n",
    "    cu_ref_count = cp.zeros(m, dtype=np.int32)\n",
    "    G = call_genotype[:]\n",
    "    t=time.time()\n",
    "    cu_arr = cp.array(G)\n",
    "    t_transfer = time.time() - t\n",
    "    \n",
    "    j = cp.zeros(1,dtype=cp.uint32)\n",
    "    v_size=call_genotype.shape[0]\n",
    "    s_size=call_genotype.shape[1]\n",
    "\n",
    "    start_gpu.record()\n",
    "    cu_count_genotypes_chunk[math.ceil(v_size/threads_per_block),threads_per_block](j, cu_arr, cu_hom_ref, cu_hom_alt, cu_het, cu_ref_count,v_size,s_size)\n",
    "    end_gpu.record()\n",
    "    end_gpu.synchronize()\n",
    "    t_compute = cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
    "\n",
    "    return cu_GenotypeCounts(cu_hom_ref, cu_hom_alt, cu_het, cu_ref_count), t_compute, t_transfer\n",
    "\n",
    "def cu_zarr_afdist(root, num_bins=10, variant_slice=None, sample_slice=None):\n",
    "    call_genotype = root[\"call_genotype\"]\n",
    "\n",
    "    n = call_genotype.shape[1]\n",
    "   \n",
    "    counts, tc, tt = cu_classify_genotypes(call_genotype)\n",
    "    \n",
    "    alt_count = 2 * n - counts.ref_count\n",
    "    af = alt_count / (n * 2)\n",
    "    bins = cp.linspace(0, 1.0, num_bins + 1)\n",
    "    bins[-1] += 0.0125\n",
    "    pRA = 2 * af * (1 - af)\n",
    "    pAA = af * af\n",
    "    x = cp.digitize(cp.array(pRA).astype(np.float32), cp.array(bins).astype(np.float32))\n",
    "    a = cp.bincount(x, weights=counts.het, minlength=num_bins + 1)\n",
    "    y = cp.digitize(cp.array(pAA).astype(np.float32), cp.array(bins).astype(np.float32))\n",
    "    b = cp.bincount(\n",
    "        y, weights=counts.hom_alt, minlength=num_bins + 1\n",
    "    )\n",
    "    count = (a + b).astype(int)\n",
    "\n",
    "    return pd.DataFrame({\"start\": bins[:-1].get(), \"stop\": bins[1:].get(), \"prob_dist\": count[1:].get()}), tc, tt\n",
    "\n",
    "\n",
    "z_root = zarr.open(dpath + \"real_data/data/WGS/chr22.zarr\",mode='r')\n",
    "\n",
    "#warm up\n",
    "cu_zarr_afdist(z_root)\n",
    "gc, t1, t2 = cu_zarr_afdist(z_root)\n",
    "\n",
    "print(gc)\n",
    "print(\"Summarisation Time (ms) = {:.3f}\".format(t1))\n",
    "print(\"Host to GPU Transfer Time (ms) = {:.3f}\".format(t2*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633f1c4-a286-47f0-925e-8fd7b3643662",
   "metadata": {},
   "source": [
    "Again, an improvement in latency is acheived. However, each instance of the kernel is having to iterate over all samples so, to get the full benefit of the GPUs threads, the kernel laucnh is set to create as many threads as there are combinations of variant and sample. The GPU's scheduler will then find the most efficient way of allocating the computation to free threads.\n",
    "\n",
    "In this example, an abstraction is used to map the 2 dimensions of the array to be summarised to 2 dimensions of threads. This makes the kernel a little simpler to write, since it can be thought of as variants in the x dimension and samples in the y dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3971117-eeb7-4608-a7d2-1f94d108dc14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/cupyx/jit/_interface.py:173: FutureWarning: cupyx.jit.rawkernel is experimental. The interface can change in the future.\n",
      "  cupy._util.experimental('cupyx.jit.rawkernel')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   start    stop  prob_dist\n",
      "0    0.0  0.1000    7081263\n",
      "1    0.1  0.2000    2212177\n",
      "2    0.2  0.3000    2096409\n",
      "3    0.3  0.4000    3027269\n",
      "4    0.4  0.5000   15854472\n",
      "5    0.5  0.6000    1769666\n",
      "6    0.6  0.7000     276639\n",
      "7    0.7  0.8000     223031\n",
      "8    0.8  0.9000     112195\n",
      "9    0.9  1.0125     488105\n",
      "Summarisation Time (ms) = 6.342\n",
      "Host to GPU Transfer Time (ms) = 138.196\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from cupyx.jit import atomic_add\n",
    "from cupyx import jit\n",
    "import cupy as cp\n",
    "\n",
    "# This value can be changed. \n",
    "threads_per_block = 8\n",
    "\n",
    "@jit.rawkernel()\n",
    "def cu_count_genotypes_chunk(G, hom_ref, hom_alt, het, ref_count,vs_size):\n",
    "    # Get the index of the current thread within a 2D block\n",
    "    thread_idx_x = jit.threadIdx.x # The x dimension is along the variant axis\n",
    "    thread_idx_y = jit.threadIdx.y # The y dimension is along the sample axis\n",
    "    \n",
    "    # The combination of block size, block index and thead index provide the variant and sample indices\n",
    "    variant_idx = jit.blockIdx.x * jit.blockDim.x + thread_idx_x \n",
    "    sample_idx = jit.blockIdx.y * jit.blockDim.y + thread_idx_y\n",
    "    \n",
    "    # because we round the thread block size up to the problem size, check the bounds\n",
    "    if variant_idx<vs_size[0]:\n",
    "        if sample_idx < vs_size[1]:\n",
    "            a = G[variant_idx, sample_idx, 0]\n",
    "            b = G[variant_idx, sample_idx, 1]\n",
    "            \n",
    "            if a == b:\n",
    "                if a == 0:\n",
    "                    atomic_add(hom_ref,variant_idx,1)\n",
    "                else:\n",
    "                    atomic_add(hom_alt,variant_idx,1)\n",
    "            else:\n",
    "                atomic_add(het,variant_idx,1)\n",
    "\n",
    "            atomic_add(ref_count,variant_idx,(a == 0))\n",
    "            atomic_add(ref_count,variant_idx,(b == 0))\n",
    "                    \n",
    "def cu_classify_genotypes(call_genotype):\n",
    "    m = call_genotype.shape[0]\n",
    "\n",
    "    # allocate cupy arrays for the results\n",
    "    cu_het = cp.zeros(m, dtype=np.int32)\n",
    "    cu_hom_alt = cp.zeros(m, dtype=np.int32)\n",
    "    cu_hom_ref = cp.zeros(m, dtype=np.int32)\n",
    "    cu_ref_count = cp.zeros(m, dtype=np.int32)\n",
    "\n",
    "    # use a cupy array to pass in the array sizes to the cuda kernel\n",
    "    G = call_genotype[:]\n",
    "    t = time.time()\n",
    "    cu_G = cp.array(G)\n",
    "    t_transfer = time.time()-t\n",
    "    vs_size = cp.zeros(2,cp.uint32)\n",
    "    vs_size[0]=cu_G.shape[0]\n",
    "    vs_size[1]=cu_G.shape[1]\n",
    "    \n",
    "    # Set the 'grid' size to the number of elements in the two array dimensions: variants and samples\n",
    "    grid_size = (math.ceil(vs_size[0]/threads_per_block),math.ceil(vs_size[1]/threads_per_block))\n",
    "    block_size = (threads_per_block,threads_per_block)\n",
    "    \n",
    "    start_gpu.record()\n",
    "    # call the cupyx kernel, with the grid size setting the number of instances of the kernel to use\n",
    "    cu_count_genotypes_chunk[grid_size,block_size](cu_G, cu_hom_ref, cu_hom_alt, cu_het, cu_ref_count,vs_size)\n",
    "    end_gpu.record()\n",
    "    end_gpu.synchronize()\n",
    "    t_compute = cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
    "    \n",
    "    return cu_GenotypeCounts(cu_hom_ref, cu_hom_alt, cu_het, cu_ref_count), t_compute, t_transfer\n",
    "\n",
    "def cu_zarr_afdist(root, num_bins=10, variant_slice=None, sample_slice=None):\n",
    "    \n",
    "    call_genotype = root[\"call_genotype\"]\n",
    "\n",
    "    n = call_genotype.shape[1]\n",
    "    counts, t1, t2 = cu_classify_genotypes(call_genotype)\n",
    "    \n",
    "    alt_count = 2 * n - counts.ref_count\n",
    "    af = alt_count / (n * 2)\n",
    "    bins = cp.linspace(0, 1.0, num_bins + 1)\n",
    "    bins[-1] += 0.0125\n",
    "    pRA = 2 * af * (1 - af)\n",
    "    pAA = af * af\n",
    "    x = cp.digitize(cp.array(pRA).astype(np.float32), cp.array(bins).astype(np.float32))\n",
    "    a = cp.bincount(x, weights=counts.het, minlength=num_bins + 1)\n",
    "    y = cp.digitize(cp.array(pAA).astype(np.float32), cp.array(bins).astype(np.float32))\n",
    "    b = cp.bincount(\n",
    "        y, weights=counts.hom_alt, minlength=num_bins + 1\n",
    "    )\n",
    "    count = (a + b).astype(int)\n",
    "\n",
    "    # note the use of .get(), which copies data over from GPU memory to CPU memory\n",
    "    return pd.DataFrame({\"start\": bins[:-1].get(), \"stop\": bins[1:].get(), \"prob_dist\": count[1:].get()}), t1, t2\n",
    "\n",
    "\n",
    "z_root = zarr.open(dpath + \"real_data/data/WGS/chr22.zarr\",mode='r')\n",
    "t = time.time()\n",
    "gc = cu_zarr_afdist(z_root)\n",
    "\n",
    "# warm up\n",
    "cu_zarr_afdist(z_root)\n",
    "gc, t1, t2 = cu_zarr_afdist(z_root)\n",
    "\n",
    "print(gc)\n",
    "print(\"Summarisation Time (ms) = {:.3f}\".format(t1))\n",
    "print(\"Host to GPU Transfer Time (ms) = {:.3f}\".format(t2*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068afd0c-d877-4326-ad76-ea528490d542",
   "metadata": {
    "tags": []
   },
   "source": [
    "As the results show, letting the GPU do the allocation of computation, hugely reduces the execution time.\n",
    "\n",
    "To deal with larger datasets without overwhelming GPU memory, the chunking approach can be used, so that each chunk is sufficiently large to fully utilise the GPU. These chunk sizes can then be used within the Zarr storage, to maximise efficiency. Where latency needs to be minimised, chunk sizes can be reduced and separate streams created, so that as one batch is being moved into GPU memory, the other is processing the previous batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d02d1-29ab-46ca-a8a7-101a4eea6f1b",
   "metadata": {},
   "source": [
    "## Zarr Decompression Using GPU on call_AD data\n",
    "\n",
    "The same, call_AD, data has been saved into several different versions of a Zarr array - each using a different algorithm. \n",
    "The GPU arrays are being loaded into GPU memory as a cupy array whereas the standard CPU Zarr data is being loaded into a numpy array.\n",
    "The data (int16) is the same size as the call_genotype (96514, 2504, 2) but has a 3rd dimension size of 7 rather than 2\n",
    "\n",
    "N.B. GPUDirect Storage has not been used here, which provides direct memory transfers from NVMe to GOU memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba4707f-ba7c-491a-941c-635efb903797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96514, 2504, 7)\n",
      "GPU Bitcomp compress time = 1.372 (s)\n",
      "GPU Snappy compress time = 0.996 (s)\n",
      "GPU LZ4 compress time = 0.937 (s)\n",
      "GPU Cascaded compress time = 0.703 (s)\n",
      "GPU Gdeflate compress time = 0.955 (s)\n"
     ]
    }
   ],
   "source": [
    "import kvikio\n",
    "import cupy as cp\n",
    "import zarr\n",
    "import kvikio.zarr\n",
    "import time\n",
    "\n",
    "def compress_nv(data, comp, root):\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    z1 = zarr.array(cp_call_ad,\n",
    "        chunks=(20000,cp_call_ad.shape[1],2),\n",
    "        store=root,\n",
    "        meta_array=cp.empty(()),\n",
    "        compressor=comp,\n",
    "        overwrite=True)\n",
    "    \n",
    "    return time.time()-t\n",
    "\n",
    "z = zarr.open(dpath + \"real_data/data/WGS/chr22.zarr\", mode='r')\n",
    "\n",
    "cp_call_ad=cp.array(z['call_AD'][:])\n",
    "print(cp_call_ad.shape)\n",
    "\n",
    "root = kvikio.zarr.GDSStore(dpath + \"real_data/data/WGS/chr22_cuda_gt_bitcomp.zarr\", normalize_keys=True)\n",
    "t = compress_nv(cp_call_ad, kvikio.zarr.Bitcomp(), root)\n",
    "print(\"GPU Bitcomp compress time = {:.3f} (s)\".format(t))\n",
    "\n",
    "root = kvikio.zarr.GDSStore(dpath + \"real_data/data/WGS/chr22_cuda_gt_snappy.zarr\", normalize_keys=True)\n",
    "t = compress_nv(cp_call_ad, kvikio.zarr.Snappy(),root)\n",
    "print(\"GPU Snappy compress time = {:.3f} (s)\".format(t))\n",
    "\n",
    "root = kvikio.zarr.GDSStore(dpath + \"real_data/data/WGS/chr22_cuda_gt_lz4.zarr\", normalize_keys=True)\n",
    "t = compress_nv(cp_call_ad, kvikio.zarr.LZ4(),root)\n",
    "print(\"GPU LZ4 compress time = {:.3f} (s)\".format(t))\n",
    "\n",
    "root = kvikio.zarr.GDSStore(dpath + \"real_data/data/WGS/chr22_cuda_gt_casc.zarr\", normalize_keys=True)\n",
    "t = compress_nv(cp_call_ad, kvikio.zarr.Cascaded(),root)\n",
    "print(\"GPU Cascaded compress time = {:.3f} (s)\".format(t))\n",
    "\n",
    "root = kvikio.zarr.GDSStore(dpath + \"real_data/data/WGS/chr22_cuda_gt_gdeflate.zarr\", normalize_keys=True)\n",
    "t = compress_nv(cp_call_ad, kvikio.zarr.Gdeflate(),root)\n",
    "print(\"GPU Gdeflate compress time = {:.3f} (s)\".format(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be65a7c-d69f-48db-843f-34200514a3c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Data has been compressed using the GPU (no comparison is made with CPU compression time here, but GPU saving and compression can be very quick). There are several different compression types, with each suited to a different data type (see [here](https://developer.nvidia.com/nvcomp))\n",
    "\n",
    "Now the same compressed Zarr files can be loaded and the time measured and compared to the CPU Zarr version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25a22f-21c2-46cd-93b2-e144d10874d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU snappy deflate time =  0.606 (s)\n",
      "GPU bitcomp deflate time =  0.626 (s)\n",
      "GPU cascade deflate time =  0.574 (s)\n",
      "GPU gdeflate deflate time =  0.967 (s)\n",
      "GPU lz4 deflate time =  0.567 (s)\n",
      "CPU zarr time =  7.560 (s)\n"
     ]
    }
   ],
   "source": [
    "import kvikio\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import zarr\n",
    "import kvikio.zarr\n",
    "import time\n",
    "\n",
    "# To prevent initialisation skewing the result, load both a CPU and GPU zarr library\n",
    "z = kvikio.zarr.open_cupy_array(store=\"real_data/data/WGS/chr22_cuda_ad_snappy.zarr\", mode='r')\n",
    "a=z[:]\n",
    "del(a)\n",
    "z1 = zarr.open_group(store=\"real_data/data/WGS/chr22.zarr\", mode='r')\n",
    "arr2 = cp.array(z1[\"call_genotype\"][:])\n",
    "del(arr2)\n",
    "# Data is deleted and then the actual measurements start\n",
    "\n",
    "t = time.time()\n",
    "z = kvikio.zarr.open_cupy_array(store=\"real_data/data/WGS/chr22_cuda_ad_bitcomp.zarr\", mode='r')\n",
    "a=z[:]\n",
    "print(\"GPU Bitcomp deflate time = {:.3f} (s)\".format(time.time()-t))\n",
    "del(a)\n",
    "\n",
    "t = time.time()\n",
    "z = kvikio.zarr.open_cupy_array(store=\"real_data/data/WGS/chr22_cuda_ad_snappy.zarr\", mode='r')\n",
    "a=z[:]\n",
    "print(\"GPU Snappy deflate time = {:.3f} (s)\".format(time.time()-t))\n",
    "del(a)\n",
    "\n",
    "t = time.time()\n",
    "z = kvikio.zarr.open_cupy_array(store=\"real_data/data/WGS/chr22_cuda_ad_lz4.zarr\", mode='r')\n",
    "a=z[:]\n",
    "print(\"GPU LZ4 deflate time = {:.3f} (s)\".format(time.time()-t))\n",
    "del(a)\n",
    "\n",
    "t = time.time()    \n",
    "z = kvikio.zarr.open_cupy_array(store=\"real_data/data/WGS/chr22_cuda_ad_casc.zarr\", mode='r')\n",
    "a=z[:]\n",
    "print(\"GPU Cascaded deflate time = {:.3f} (s)\".format(time.time()-t))\n",
    "del(a)\n",
    "\n",
    "t = time.time()\n",
    "z = kvikio.zarr.open_cupy_array(store=\"real_data/data/WGS/chr22_cuda_ad_gdeflate.zarr\", mode='r')\n",
    "a=z[:]\n",
    "print(\"GPU Gdeflate deflate time = {:.3f} (s)\".format(time.time()-t))\n",
    "del(a)\n",
    "\n",
    "t = time.time()\n",
    "z1 = zarr.open_group(store=\"real_data/data/WGS/chr22.zarr\", mode='r')\n",
    "arr2 = z1[\"call_AD\"][:]\n",
    "\n",
    "print(\"CPU zarr time = \", time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f4c82d-ad48-46f0-a33f-273f6b6f739e",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix\n",
    "\n",
    "For some kernels, it can be worth using the Shared Memory feature of a GPU. Shared Memory (only visible to threads within the same block) is faster than global memory (visibile to all threads), but is limited.\n",
    "In this case it makes little or no difference, but for other tasks in which memory is being accessed repeatedly, it can offer noticeable speed ups. For reference, below is an implementation of the kernel that uses Shared Memory.\n",
    "For more information in cupy kernels, see [here](https://docs.cupy.dev/en/stable/user_guide/kernel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04540b8b-6ad8-4f62-86f9-2e75a52125cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit.rawkernel()\n",
    "def cu_count_genotypes_chunk(G, hom_ref, hom_alt, het, ref_count,vs_size):\n",
    "    # Get the index of the current thread within a 2D block\n",
    "    thread_idx_x = jit.threadIdx.x # The x dimension is along the variant axis\n",
    "    thread_idx_y = jit.threadIdx.y # The y dimension is along the sample axis\n",
    "    \n",
    "    # The combination of block size, block index and thead index provide the  variant and sample indexes\n",
    "    variant_idx = jit.blockIdx.x * jit.blockDim.x + thread_idx_x \n",
    "    sample_idx = jit.blockIdx.y * jit.blockDim.y + thread_idx_y\n",
    "    sm_offset = (thread_idx_x*threads_per_block+thread_idx_y)*4\n",
    "    \n",
    "    # Shared memory is shared between threads within the same block\n",
    "    # It is limited in size but faster than global memory so can be used to cache data\n",
    "    shmem = jit.shared_memory(cp.int32,threads_per_block*threads_per_block*4)\n",
    "    \n",
    "    # Initialise the shared memory for the cached counts\n",
    "    shmem[sm_offset + 0]=0 # used for hom_ref\n",
    "    shmem[sm_offset + 1]=0 # used for hom_alt\n",
    "    shmem[sm_offset + 2]=0 # used for het\n",
    "    shmem[sm_offset + 3]=0 # used for ref_count\n",
    "    \n",
    "    # because we round the thread block size up to the problem size, check the bounds\n",
    "    if variant_idx<vs_size[0]:\n",
    "        if sample_idx < vs_size[1]:\n",
    "            a = G[variant_idx, sample_idx, 0]\n",
    "            b = G[variant_idx, sample_idx, 1]\n",
    "            if a == b:\n",
    "                if a == 0:\n",
    "                    shmem[sm_offset] += 1\n",
    "                else:\n",
    "                    shmem[sm_offset + 1] += 1\n",
    "            else:\n",
    "                shmem[sm_offset + 2] += 1\n",
    "            shmem[sm_offset + 3] += (a==0)\n",
    "            shmem[sm_offset + 3] += (b==0)\n",
    "    \n",
    "    # Now add the final shared memory cache vakues to the\n",
    "    # global memory arrays that were passed in\n",
    "    # using atomic adds to avoid data races\n",
    "    atomic_add(hom_ref,variant_idx,shmem[sm_offset])\n",
    "    atomic_add(hom_alt,variant_idx, shmem[sm_offset+1])\n",
    "    atomic_add(het,variant_idx, shmem[sm_offset+2])\n",
    "    atomic_add(ref_count,variant_idx, shmem[sm_offset+3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
