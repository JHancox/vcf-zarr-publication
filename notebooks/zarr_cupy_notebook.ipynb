{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "909cbb40-9b27-4a45-8acb-e229277cf219",
   "metadata": {},
   "source": [
    "## Comparison of CPU and GPU accelerated Genotype counting\n",
    "\n",
    "Firstly here is a CPU baseline for the computation of call_genotype statistics. This is a simplified version of the code in zarr_afdist.py.\n",
    "\n",
    "In this case, the data (int8) that is being loaded is comprised of 26504 variants across 2504 diploid samples.\n",
    "\n",
    "The data is loaded using Zarr using an Intel Xeon Silver 4216 x 64 CPU and Samsung NVMe drive (SSD 970 EVO Plus 2TB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e694f-1bab-4626-8dec-680059ee1b31",
   "metadata": {
    "tags": []
   },
   "source": [
    "!vcf2zarr inspect chr22.zarr yields the following information about the chr22.zarr vcf archive:\n",
    "\n",
    "| name                          dtype    stored      size          ratio    nchunks  chunk_size    avg_chunk_stored    shape              chunk_shape        compressor                                                      filters |\n"
    "| ----------------------------  -------  ----------  ----------  -------  ---------  ------------  ------------------  -----------------  -----------------  --------------------------------------------------------------  ------------ |\n"
    "| /call_PL                      int32    505.95 MiB  25.21 GiB     51            30  860.44 MiB    16.86 MiB           (96514, 2504, 28)  (10000, 1000, 28)  Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /call_AD                      int16    199.34 MiB  3.15 GiB      16            30  107.56 MiB    6.64 MiB            (96514, 2504, 7)   (10000, 1000, 7)   Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /call_DP                      int16    112.17 MiB  460.95 MiB     4.1          30  15.37 MiB     3.74 MiB            (96514, 2504)      (10000, 1000)      Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /call_GQ                      int8     58.69 MiB   230.48 MiB     3.9          30  7.68 MiB      1.96 MiB            (96514, 2504)      (10000, 1000)      Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /call_AB                      float32  31.02 MiB   921.9 MiB     30            30  30.73 MiB     1.03 MiB            (96514, 2504)      (10000, 1000)      Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /call_PID                     object   9.36 MiB    1.8 GiB      200            30  61.46 MiB     319.48 KiB          (96514, 2504)      (10000, 1000)      Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   [VLenUTF8()] |\n"
    "| /call_genotype                int8     7.82 MiB    460.95 MiB    59            30  15.37 MiB     266.88 KiB          (96514, 2504, 2)   (10000, 1000, 2)   Blosc(cname='zstd', clevel=7, shuffle=BITSHUFFLE, blocksize=0)  None |\n"
    "| /call_PGT                     object   7.44 MiB    1.8 GiB      250            30  61.46 MiB     253.93 KiB          (96514, 2504)      (10000, 1000)      Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   [VLenUTF8()] |\n"
    "| /call_genotype_mask           bool     1.63 MiB    460.95 MiB   280            30  15.37 MiB     55.67 KiB           (96514, 2504, 2)   (10000, 1000, 2)   Blosc(cname='zstd', clevel=7, shuffle=BITSHUFFLE, blocksize=0)  None |\n"
    "| /variant_quality              float32  321.69 KiB  377.01 KiB     1.2          10  37.7 KiB      32.17 KiB           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_AF                   float32  290.68 KiB  2.21 MiB       7.8          10  226.2 KiB     29.07 KiB           (96514, 6)         (10000, 6)         Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_MLEAF                float32  289.18 KiB  2.21 MiB       7.8          10  226.2 KiB     28.92 KiB           (96514, 6)         (10000, 6)         Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_VQSLOD               float32  281.49 KiB  377.01 KiB     1.3          10  37.7 KiB      28.15 KiB           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_position             int32    275.26 KiB  377.01 KiB     1.4          10  37.7 KiB      27.53 KiB           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_DP                   int32    274.18 KiB  377.01 KiB     1.4          10  37.7 KiB      27.42 KiB           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_FS                   float32  247.86 KiB  377.01 KiB     1.5          10  37.7 KiB      24.79 KiB           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_allele               object   246.62 KiB  5.15 MiB      21            10  527.81 KiB    24.66 KiB           (96514, 7)         (10000, 7)         Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   [VLenUTF8()] |\n"
    "| /variant_MQRankSum            float32  244.53 KiB  377.01 KiB     1.5          10  37.7 KiB      24.45 KiB           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_SOR                  float32  227.69 KiB  377.01 KiB     1.7          10  37.7 KiB      22.77 KiB           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_ReadPosRankSum       float32  220.18 KiB  377.01 KiB     1.7          10  37.7 KiB      22.02 KiB           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_BaseQRankSum         float32  212.24 KiB  377.01 KiB     1.8          10  37.7 KiB      21.22 KiB           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_ClippingRankSum      float32  209.56 KiB  377.01 KiB     1.8          10  37.7 KiB      20.96 KiB           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_MQ                   float32  209.18 KiB  377.01 KiB     1.8          10  37.7 KiB      20.92 KiB           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_AC                   int16    209.14 KiB  1.1 MiB        5.4          10  113.1 KiB     20.91 KiB           (96514, 6)         (10000, 6)         Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_MLEAC                int16    208.79 KiB  1.1 MiB        5.4          10  113.1 KiB     20.88 KiB           (96514, 6)         (10000, 6)         Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_InbreedingCoeff      float32  205.43 KiB  377.01 KiB     1.8          10  37.7 KiB      20.54 KiB           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_QD                   float32  196.03 KiB  377.01 KiB     1.9          10  37.7 KiB      19.6 KiB            (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_ExcessHet            float32  155.89 KiB  377.01 KiB     2.4          10  37.7 KiB      15.59 KiB           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /call_RGQ                     int8     76.67 KiB   230.48 MiB  3100            30  7.68 MiB      2.56 KiB            (96514, 2504)      (10000, 1000)      Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /call_SB                      int8     76.66 KiB   230.48 MiB  3100            30  7.68 MiB      2.55 KiB            (96514, 2504)      (10000, 1000)      Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /call_MQ0                     int8     76.61 KiB   230.48 MiB  3100            30  7.68 MiB      2.55 KiB            (96514, 2504)      (10000, 1000)      Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /call_MIN_DP                  int8     76.61 KiB   230.48 MiB  3100            30  7.68 MiB      2.55 KiB            (96514, 2504)      (10000, 1000)      Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /call_genotype_phased         bool     69.11 KiB   230.48 MiB  3400            30  7.68 MiB      2.3 KiB             (96514, 2504)      (10000, 1000)      Blosc(cname='zstd', clevel=7, shuffle=BITSHUFFLE, blocksize=0)  None |\n"
    "| /variant_filter               bool     58.69 KiB   565.51 KiB     9.6          10  56.55 KiB     5.87 KiB            (96514, 6)         (10000, 6)         Blosc(cname='zstd', clevel=7, shuffle=BITSHUFFLE, blocksize=0)  None |\n"
    "| /variant_AN                   int16    42.24 KiB   188.5 KiB      4.5          10  18.85 KiB     4.22 KiB            (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_culprit              object   24.1 KiB    754.02 KiB    31            10  75.4 KiB      2.41 KiB            (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   [VLenUTF8()] |\n"
    "| /variant_NEGATIVE_TRAIN_SITE  bool     10.78 KiB   94.25 KiB      8.7          10  9.43 KiB      1.08 KiB            (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=BITSHUFFLE, blocksize=0)  None |\n"
    "| /contig_length                int64    9.24 KiB    26.3 KiB       2.8           1  26.3 KiB      9.24 KiB            (3366,)            (3366,)            Blosc(cname='zstd', clevel=7, shuffle=SHUFFLE, blocksize=0)     None |\n"
    "| /contig_id                    object   8.33 KiB    26.3 KiB       3.2           1  26.3 KiB      8.33 KiB            (3366,)            (3366,)            Blosc(cname='zstd', clevel=7, shuffle=SHUFFLE, blocksize=0)     [VLenUTF8()] |\n"
    "| /sample_id                    object   6.7 KiB     19.56 KiB      2.9           3  6.52 KiB      2.23 KiB            (2504,)            (1000,)            Blosc(cname='zstd', clevel=7, shuffle=SHUFFLE, blocksize=0)     [VLenUTF8()] |\n"
    "| /variant_VariantType          object   5 KiB       754.02 KiB   150            10  75.4 KiB      511 bytes           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   [VLenUTF8()] |\n"
    "| /variant_id                   object   4.97 KiB    754.02 KiB   150            10  75.4 KiB      509 bytes           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   [VLenUTF8()] |\n"
    "| /variant_HaplotypeScore       float32  4.94 KiB    377.01 KiB    76            10  37.7 KiB      505 bytes           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_RAW_MQ               float32  4.91 KiB    377.01 KiB    77            10  37.7 KiB      502 bytes           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_POSITIVE_TRAIN_SITE  bool     4.9 KiB     94.25 KiB     19            10  9.43 KiB      502 bytes           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=BITSHUFFLE, blocksize=0)  None |\n"
    "| /variant_id_mask              bool     4.87 KiB    94.25 KiB     19            10  9.43 KiB      498 bytes           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=BITSHUFFLE, blocksize=0)  None |\n"
    "| /variant_DS                   bool     4.87 KiB    94.25 KiB     19            10  9.43 KiB      498 bytes           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=BITSHUFFLE, blocksize=0)  None |\n"
    "| /variant_END                  int8     4.86 KiB    94.25 KiB     19            10  9.43 KiB      498 bytes           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_MQ0                  int8     4.86 KiB    94.25 KiB     19            10  9.43 KiB      497 bytes           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /variant_contig               int16    4.85 KiB    188.5 KiB     39            10  18.85 KiB     497 bytes           (96514,)           (10000,)           Blosc(cname='zstd', clevel=7, shuffle=NOSHUFFLE, blocksize=0)   None |\n"
    "| /filter_id                    object   4.52 KiB    48 bytes       0.01          1  48.0 bytes    4.52 KiB            (6,)               (6,)               Blosc(cname='zstd', clevel=7, shuffle=SHUFFLE, blocksize=0)     [VLenUTF8()] |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49aa6833-3275-4249-948e-9023f0d0d097",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   start    stop  prob_dist\n",
      "0    0.0  0.1000    7081263\n",
      "1    0.1  0.2000    2212177\n",
      "2    0.2  0.3000    2096409\n",
      "3    0.3  0.4000    3027269\n",
      "4    0.4  0.5000   15854472\n",
      "5    0.5  0.6000    1769666\n",
      "6    0.6  0.7000     276639\n",
      "7    0.7  0.8000     223031\n",
      "8    0.8  0.9000     112195\n",
      "9    0.9  1.0125     488105\n",
      "Summarisation Time (ms) = 1457.252\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba\n",
    "import zarr\n",
    "import numcodecs\n",
    "import math\n",
    "import time\n",
    "\n",
    "dpath = \"/datasets/sgkit/vcf-zarr-publication/\"\n",
    "\n",
    "@numba.njit(\"void(int64, int8[:, :, :], int32[:], int32[:], int32[:], int32[:])\")\n",
    "def count_genotypes_chunk(offset, G, hom_ref, hom_alt, het, ref_count):\n",
    "    # NB Assuming diploids and no missing data!\n",
    "    index = offset\n",
    "    for j in range(G.shape[0]):\n",
    "        for k in range(G.shape[1]):\n",
    "            a = G[j, k, 0]\n",
    "            b = G[j, k, 1]\n",
    "            if a == b:\n",
    "                if a == 0:\n",
    "                    hom_ref[index] += 1\n",
    "                else:\n",
    "                    hom_alt[index] += 1\n",
    "            else:\n",
    "                het[index] += 1\n",
    "            ref_count[index] += (a == 0) + (b == 0)\n",
    "        index += 1\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class GenotypeCounts:\n",
    "    hom_ref: list\n",
    "    hom_alt: list\n",
    "    het: list\n",
    "    ref_count: list\n",
    "\n",
    "def classify_genotypes(call_genotype):\n",
    "    m = call_genotype.shape[0]\n",
    "\n",
    "    het = np.zeros(m, dtype=np.int32)\n",
    "    hom_alt = np.zeros(m, dtype=np.int32)\n",
    "    hom_ref = np.zeros(m, dtype=np.int32)\n",
    "    ref_count = np.zeros(m, dtype=np.int32)\n",
    "    j = 0\n",
    "    t1 = 0\n",
    "    \n",
    "    for v_chunk in range(call_genotype.cdata_shape[0]):\n",
    "        for s_chunk in range(call_genotype.cdata_shape[1]):\n",
    "            G = call_genotype.blocks[v_chunk, s_chunk]\n",
    "            t = time.time()\n",
    "            count_genotypes_chunk(j, G, hom_ref, hom_alt, het, ref_count)\n",
    "            t1 += (time.time()-t)\n",
    "        j += G.shape[0]\n",
    "   \n",
    "    return GenotypeCounts(hom_ref, hom_alt, het, ref_count), t1\n",
    "\n",
    "def zarr_afdist(root, num_bins=10, variant_slice=None, sample_slice=None):\n",
    "    call_genotype = root[\"call_genotype\"]\n",
    "    m = call_genotype.shape[0]\n",
    "    n = call_genotype.shape[1]\n",
    "\n",
    "    counts, t = classify_genotypes(call_genotype)\n",
    "\n",
    "    alt_count = 2 * n - counts.ref_count\n",
    "    af = alt_count / (n * 2)\n",
    "    bins = np.linspace(0, 1.0, num_bins + 1)\n",
    "    bins[-1] += 0.0125\n",
    "    pRA = 2 * af * (1 - af)\n",
    "    pAA = af * af\n",
    "\n",
    "    a = np.bincount(np.digitize(pRA, bins), weights=counts.het, minlength=num_bins + 1)\n",
    "    b = np.bincount(\n",
    "        np.digitize(pAA, bins), weights=counts.hom_alt, minlength=num_bins + 1\n",
    "    )\n",
    "\n",
    "    count = (a + b).astype(int)\n",
    "\n",
    "    return pd.DataFrame({\"start\": bins[:-1], \"stop\": bins[1:], \"prob_dist\": count[1:]}), t\n",
    "\n",
    "z_root = zarr.open(dpath + \"real_data/data/WGS/chr22.zarr\",mode='r')\n",
    "\n",
    "# warm up\n",
    "zarr_afdist(z_root)\n",
    "gc, t = zarr_afdist(z_root)\n",
    "\n",
    "print(gc)\n",
    "print(\"Summarisation Time (ms) = {:.3f}\".format(t*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a34a5-7d25-48bf-8563-c9805e23dc73",
   "metadata": {},
   "source": [
    "Note that what is measured in \"Summarisation Time\" is the time to compute the summarisation only (i.e. no loading the data)\n",
    "\n",
    "Next, the same computation is done using a cupy array and a cupy kernel. cupy arrays are functionally the same as numpy arrays, but reside within GPU memory.\n",
    "\n",
    "The GPU used is an Nvidia A6000 RTX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1258784-c0c5-459a-bdcc-31bd200abfd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/cupyx/jit/_interface.py:173: FutureWarning: cupyx.jit.rawkernel is experimental. The interface can change in the future.\n",
      "  cupy._util.experimental('cupyx.jit.rawkernel')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   start    stop  prob_dist\n",
      "0    0.0  0.1000    7081263\n",
      "1    0.1  0.2000    2212177\n",
      "2    0.2  0.3000    2096409\n",
      "3    0.3  0.4000    3027269\n",
      "4    0.4  0.5000   15854472\n",
      "5    0.5  0.6000    1769666\n",
      "6    0.6  0.7000     276639\n",
      "7    0.7  0.8000     223031\n",
      "8    0.8  0.9000     112195\n",
      "9    0.9  1.0125     488105\n",
      "Summarisation Time (ms) = 24.684\n",
      "Host to GPU Transfer Time (ms) = 179.458\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import time\n",
    "from cupyx import jit\n",
    "\n",
    "start_gpu = cp.cuda.Event()\n",
    "end_gpu = cp.cuda.Event()\n",
    "\n",
    "threads_per_block = 256\n",
    "\n",
    "@jit.rawkernel()\n",
    "def cu_count_genotypes_chunk(offset, G, hom_ref, hom_alt, het, ref_count, vs_size):\n",
    "    # NB Assuming diploids and no missing data!\n",
    "    idx = jit.blockIdx.x * jit.blockDim.x + jit.threadIdx.x\n",
    "    \n",
    "    variant_idx = idx + offset[0]\n",
    "\n",
    "    if idx<vs_size[0]:\n",
    "        for k in range(vs_size[1]):\n",
    "            a = G[idx, k, 0]\n",
    "            b = G[idx, k, 1]\n",
    "            if a == b:\n",
    "                if a == 0:\n",
    "                    hom_ref[variant_idx] += 1\n",
    "                else:\n",
    "                    hom_alt[variant_idx] += 1\n",
    "            else:\n",
    "                het[variant_idx] += 1\n",
    "\n",
    "            ref_count[variant_idx] += (a == 0)\n",
    "            ref_count[variant_idx] += (b == 0)\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class cu_GenotypeCounts:\n",
    "    hom_ref: 'np.ndarray[np.int32]'\n",
    "    hom_alt: 'np.ndarray[np.int32]'\n",
    "    het: 'np.ndarray[np.int32]'\n",
    "    ref_count: 'np.ndarray[np.int32]'\n",
    "    \n",
    "def cu_classify_genotypes_chunked(call_genotype):\n",
    "    m = call_genotype.shape[0]\n",
    "\n",
    "    cu_het = cp.zeros(m, dtype=np.int32)\n",
    "    cu_hom_alt = cp.zeros(m, dtype=np.int32)\n",
    "    cu_hom_ref = cp.zeros(m, dtype=np.int32)\n",
    "    cu_ref_count = cp.zeros(m, dtype=np.int32)\n",
    "    t_compute=0\n",
    "    t_transfer=0\n",
    "    \n",
    "    j = cp.zeros(1,dtype=cp.uint32)\n",
    "    for v_chunk in range(call_genotype.cdata_shape[0]):\n",
    "        for s_chunk in range(call_genotype.cdata_shape[1]):\n",
    "            # measure transfer time\n",
    "            G = call_genotype.blocks[v_chunk, s_chunk]\n",
    "            t = time.time()\n",
    "            cu_G = cp.array(G)\n",
    "            t_transfer += (time.time()-t)\n",
    "            vs_size = cp.zeros(2,cp.uint32)\n",
    "            vs_size[0] = cu_G.shape[0]\n",
    "            vs_size[1] = cu_G.shape[1]\n",
    "            start_gpu.record()\n",
    "            cu_count_genotypes_chunk[math.ceil(vs_size[0]/threads_per_block),threads_per_block](j, cu_G, cu_hom_ref, cu_hom_alt, cu_het, cu_ref_count,vs_size)\n",
    "            end_gpu.record()\n",
    "            end_gpu.synchronize()\n",
    "            t_compute += cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
    "        j += cu_G.shape[0]\n",
    "        \n",
    "    return cu_GenotypeCounts(cu_hom_ref, cu_hom_alt, cu_het, cu_ref_count), t_compute, t_transfer\n",
    "\n",
    "def cu_zarr_afdist(root, num_bins=10, variant_slice=None, sample_slice=None):\n",
    "    call_genotype = root[\"call_genotype\"]\n",
    "\n",
    "    n = call_genotype.shape[1]\n",
    "    counts, t1, t2 = cu_classify_genotypes_chunked(call_genotype)\n",
    "    alt_count = 2 * n - counts.ref_count\n",
    "    af = alt_count / (n * 2)\n",
    "    bins = cp.linspace(0, 1.0, num_bins + 1)\n",
    "    bins[-1] += 0.0125\n",
    "    pRA = 2 * af * (1 - af)\n",
    "    pAA = af * af\n",
    "    x = cp.digitize(cp.array(pRA).astype(np.float32), cp.array(bins).astype(np.float32))\n",
    "    a = cp.bincount(x, weights=counts.het, minlength=num_bins + 1)\n",
    "    y = cp.digitize(cp.array(pAA).astype(np.float32), cp.array(bins).astype(np.float32))\n",
    "    b = cp.bincount(\n",
    "        y, weights=counts.hom_alt, minlength=num_bins + 1\n",
    "    )\n",
    "    count = (a + b).astype(int)\n",
    "\n",
    "    return pd.DataFrame({\"start\": bins[:-1].get(), \"stop\": bins[1:].get(), \"prob_dist\": count[1:].get()}), t1, t2\n",
    "\n",
    "\n",
    "z_root = zarr.open(dpath + \"real_data/data/WGS/chr22.zarr\",mode='r')\n",
    "\n",
    "# warm up\n",
    "cu_zarr_afdist(z_root)\n",
    "gc, t1, t2 = cu_zarr_afdist(z_root)\n",
    "\n",
    "print(gc)\n",
    "print(\"Summarisation Time (ms) = {:.3f}\".format(t1))\n",
    "print(\"Host to GPU Transfer Time (ms) = {:.3f}\".format(t2*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6437d682-9b68-476c-b32d-de07417ff108",
   "metadata": {},
   "source": [
    "This simple kernel produces a modest speed-up, but there is more that can be done to optimise the cupy kernel.\n",
    "\n",
    "Next, the data is passed into the kernel in one large chunk and shared memory is used to cache interim results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4d3116e-3c0d-43e6-a05e-e939a6578f09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/cupyx/jit/_interface.py:173: FutureWarning: cupyx.jit.rawkernel is experimental. The interface can change in the future.\n",
      "  cupy._util.experimental('cupyx.jit.rawkernel')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   start    stop  prob_dist\n",
      "0    0.0  0.1000    7081263\n",
      "1    0.1  0.2000    2212177\n",
      "2    0.2  0.3000    2096409\n",
      "3    0.3  0.4000    3027269\n",
      "4    0.4  0.5000   15854472\n",
      "5    0.5  0.6000    1769666\n",
      "6    0.6  0.7000     276639\n",
      "7    0.7  0.8000     223031\n",
      "8    0.8  0.9000     112195\n",
      "9    0.9  1.0125     488105\n",
      "Summarisation Time (ms) = 21.120\n",
      "Host to GPU Transfer Time (ms) = 135.895\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import time\n",
    "from cupyx import jit\n",
    "\n",
    "threads_per_block = 64\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class cu_GenotypeCounts:\n",
    "    hom_ref: 'np.ndarray[np.int32]'\n",
    "    hom_alt: 'np.ndarray[np.int32]'\n",
    "    het: 'np.ndarray[np.int32]'\n",
    "    ref_count: 'np.ndarray[np.int32]'\n",
    "\n",
    "\n",
    "@jit.rawkernel()\n",
    "def cu_count_genotypes_chunk(variant_offset, G, hom_ref, hom_alt, het, ref_count,v_size,s_size):\n",
    "    # NB Assuming diploids and no missing data!\n",
    "    thread_idx = jit.threadIdx.x\n",
    "    grid_idx = jit.blockIdx.x * jit.blockDim.x + thread_idx\n",
    "    sm_offset = thread_idx*4\n",
    "    \n",
    "    variant_idx = grid_idx + variant_offset[0]\n",
    "\n",
    "    if grid_idx<v_size:\n",
    "        for k in range(s_size):\n",
    "            a = G[grid_idx, k, 0]\n",
    "            b = G[grid_idx, k, 1]\n",
    "            \n",
    "            if a == b:\n",
    "                if a == 0:\n",
    "                    hom_ref[variant_idx] += 1\n",
    "                else:\n",
    "                    hom_alt[variant_idx] += 1\n",
    "            else:\n",
    "                het[variant_idx] += 1\n",
    "            ref_count[variant_idx] += (a == 0)\n",
    "            ref_count[variant_idx] += (b == 0)\n",
    "    \n",
    "                    \n",
    "def cu_classify_genotypes(call_genotype):\n",
    "    m = call_genotype.shape[0]\n",
    "\n",
    "    cu_het = cp.zeros(m, dtype=np.int32)\n",
    "    cu_hom_alt = cp.zeros(m, dtype=np.int32)\n",
    "    cu_hom_ref = cp.zeros(m, dtype=np.int32)\n",
    "    cu_ref_count = cp.zeros(m, dtype=np.int32)\n",
    "    G = call_genotype[:]\n",
    "    t=time.time()\n",
    "    cu_arr = cp.array(G)\n",
    "    t_transfer = time.time() - t\n",
    "    \n",
    "    j = cp.zeros(1,dtype=cp.uint32)\n",
    "    v_size=call_genotype.shape[0]\n",
    "    s_size=call_genotype.shape[1]\n",
    "\n",
    "    start_gpu.record()\n",
    "    cu_count_genotypes_chunk[math.ceil(v_size/threads_per_block),threads_per_block](j, cu_arr, cu_hom_ref, cu_hom_alt, cu_het, cu_ref_count,v_size,s_size)\n",
    "    end_gpu.record()\n",
    "    end_gpu.synchronize()\n",
    "    t_compute = cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
    "\n",
    "    return cu_GenotypeCounts(cu_hom_ref, cu_hom_alt, cu_het, cu_ref_count), t_compute, t_transfer\n",
    "\n",
    "def cu_zarr_afdist(root, num_bins=10, variant_slice=None, sample_slice=None):\n",
    "    call_genotype = root[\"call_genotype\"]\n",
    "\n",
    "    n = call_genotype.shape[1]\n",
    "   \n",
    "    counts, tc, tt = cu_classify_genotypes(call_genotype)\n",
    "    \n",
    "    alt_count = 2 * n - counts.ref_count\n",
    "    af = alt_count / (n * 2)\n",
    "    bins = cp.linspace(0, 1.0, num_bins + 1)\n",
    "    bins[-1] += 0.0125\n",
    "    pRA = 2 * af * (1 - af)\n",
    "    pAA = af * af\n",
    "    x = cp.digitize(cp.array(pRA).astype(np.float32), cp.array(bins).astype(np.float32))\n",
    "    a = cp.bincount(x, weights=counts.het, minlength=num_bins + 1)\n",
    "    y = cp.digitize(cp.array(pAA).astype(np.float32), cp.array(bins).astype(np.float32))\n",
    "    b = cp.bincount(\n",
    "        y, weights=counts.hom_alt, minlength=num_bins + 1\n",
    "    )\n",
    "    count = (a + b).astype(int)\n",
    "\n",
    "    return pd.DataFrame({\"start\": bins[:-1].get(), \"stop\": bins[1:].get(), \"prob_dist\": count[1:].get()}), tc, tt\n",
    "\n",
    "\n",
    "z_root = zarr.open(dpath + \"real_data/data/WGS/chr22.zarr\",mode='r')\n",
    "\n",
    "#warm up\n",
    "cu_zarr_afdist(z_root)\n",
    "gc, t1, t2 = cu_zarr_afdist(z_root)\n",
    "\n",
    "print(gc)\n",
    "print(\"Summarisation Time (ms) = {:.3f}\".format(t1))\n",
    "print(\"Host to GPU Transfer Time (ms) = {:.3f}\".format(t2*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633f1c4-a286-47f0-925e-8fd7b3643662",
   "metadata": {},
   "source": [
    "Again, an improvement in latency is acheived. However, each instance of the kernel is having to iterate over all samples so, to get the full benefit of the GPUs threads, the kernel laucnh is set to create as many threads as there are combinations of variant and sample. The GPU's scheduler will then find the most efficient way of allocating the computation to free threads.\n",
    "\n",
    "In this example, an abstraction is used to map the 2 dimensions of the array to be summarised to 2 dimensions of threads. This makes the kernel a little simpler to write, since it can be thought of as variants in the x dimension and samples in the y dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3971117-eeb7-4608-a7d2-1f94d108dc14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/cupyx/jit/_interface.py:173: FutureWarning: cupyx.jit.rawkernel is experimental. The interface can change in the future.\n",
      "  cupy._util.experimental('cupyx.jit.rawkernel')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   start    stop  prob_dist\n",
      "0    0.0  0.1000    7081263\n",
      "1    0.1  0.2000    2212177\n",
      "2    0.2  0.3000    2096409\n",
      "3    0.3  0.4000    3027269\n",
      "4    0.4  0.5000   15854472\n",
      "5    0.5  0.6000    1769666\n",
      "6    0.6  0.7000     276639\n",
      "7    0.7  0.8000     223031\n",
      "8    0.8  0.9000     112195\n",
      "9    0.9  1.0125     488105\n",
      "Summarisation Time (ms) = 6.342\n",
      "Host to GPU Transfer Time (ms) = 138.196\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from cupyx.jit import atomic_add\n",
    "from cupyx import jit\n",
    "import cupy as cp\n",
    "\n",
    "# This value can be changed. \n",
    "threads_per_block = 8\n",
    "\n",
    "@jit.rawkernel()\n",
    "def cu_count_genotypes_chunk(G, hom_ref, hom_alt, het, ref_count,vs_size):\n",
    "    # Get the index of the current thread within a 2D block\n",
    "    thread_idx_x = jit.threadIdx.x # The x dimension is along the variant axis\n",
    "    thread_idx_y = jit.threadIdx.y # The y dimension is along the sample axis\n",
    "    \n",
    "    # The combination of block size, block index and thead index provide the variant and sample indices\n",
    "    variant_idx = jit.blockIdx.x * jit.blockDim.x + thread_idx_x \n",
    "    sample_idx = jit.blockIdx.y * jit.blockDim.y + thread_idx_y\n",
    "    \n",
    "    # because we round the thread block size up to the problem size, check the bounds\n",
    "    if variant_idx<vs_size[0]:\n",
    "        if sample_idx < vs_size[1]:\n",
    "            a = G[variant_idx, sample_idx, 0]\n",
    "            b = G[variant_idx, sample_idx, 1]\n",
    "            \n",
    "            if a == b:\n",
    "                if a == 0:\n",
    "                    atomic_add(hom_ref,variant_idx,1)\n",
    "                else:\n",
    "                    atomic_add(hom_alt,variant_idx,1)\n",
    "            else:\n",
    "                atomic_add(het,variant_idx,1)\n",
    "\n",
    "            atomic_add(ref_count,variant_idx,(a == 0))\n",
    "            atomic_add(ref_count,variant_idx,(b == 0))\n",
    "                    \n",
    "def cu_classify_genotypes(call_genotype):\n",
    "    m = call_genotype.shape[0]\n",
    "\n",
    "    # allocate cupy arrays for the results\n",
    "    cu_het = cp.zeros(m, dtype=np.int32)\n",
    "    cu_hom_alt = cp.zeros(m, dtype=np.int32)\n",
    "    cu_hom_ref = cp.zeros(m, dtype=np.int32)\n",
    "    cu_ref_count = cp.zeros(m, dtype=np.int32)\n",
    "\n",
    "    # use a cupy array to pass in the array sizes to the cuda kernel\n",
    "    G = call_genotype[:]\n",
    "    t = time.time()\n",
    "    cu_G = cp.array(G)\n",
    "    t_transfer = time.time()-t\n",
    "    vs_size = cp.zeros(2,cp.uint32)\n",
    "    vs_size[0]=cu_G.shape[0]\n",
    "    vs_size[1]=cu_G.shape[1]\n",
    "    \n",
    "    # Set the 'grid' size to the number of elements in the two array dimensions: variants and samples\n",
    "    grid_size = (math.ceil(vs_size[0]/threads_per_block),math.ceil(vs_size[1]/threads_per_block))\n",
    "    block_size = (threads_per_block,threads_per_block)\n",
    "    \n",
    "    start_gpu.record()\n",
    "    # call the cupyx kernel, with the grid size setting the number of instances of the kernel to use\n",
    "    cu_count_genotypes_chunk[grid_size,block_size](cu_G, cu_hom_ref, cu_hom_alt, cu_het, cu_ref_count,vs_size)\n",
    "    end_gpu.record()\n",
    "    end_gpu.synchronize()\n",
    "    t_compute = cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
    "    \n",
    "    return cu_GenotypeCounts(cu_hom_ref, cu_hom_alt, cu_het, cu_ref_count), t_compute, t_transfer\n",
    "\n",
    "def cu_zarr_afdist(root, num_bins=10, variant_slice=None, sample_slice=None):\n",
    "    \n",
    "    call_genotype = root[\"call_genotype\"]\n",
    "\n",
    "    n = call_genotype.shape[1]\n",
    "    counts, t1, t2 = cu_classify_genotypes(call_genotype)\n",
    "    \n",
    "    alt_count = 2 * n - counts.ref_count\n",
    "    af = alt_count / (n * 2)\n",
    "    bins = cp.linspace(0, 1.0, num_bins + 1)\n",
    "    bins[-1] += 0.0125\n",
    "    pRA = 2 * af * (1 - af)\n",
    "    pAA = af * af\n",
    "    x = cp.digitize(cp.array(pRA).astype(np.float32), cp.array(bins).astype(np.float32))\n",
    "    a = cp.bincount(x, weights=counts.het, minlength=num_bins + 1)\n",
    "    y = cp.digitize(cp.array(pAA).astype(np.float32), cp.array(bins).astype(np.float32))\n",
    "    b = cp.bincount(\n",
    "        y, weights=counts.hom_alt, minlength=num_bins + 1\n",
    "    )\n",
    "    count = (a + b).astype(int)\n",
    "\n",
    "    # note the use of .get(), which copies data over from GPU memory to CPU memory\n",
    "    return pd.DataFrame({\"start\": bins[:-1].get(), \"stop\": bins[1:].get(), \"prob_dist\": count[1:].get()}), t1, t2\n",
    "\n",
    "\n",
    "z_root = zarr.open(dpath + \"real_data/data/WGS/chr22.zarr\",mode='r')\n",
    "t = time.time()\n",
    "gc = cu_zarr_afdist(z_root)\n",
    "\n",
    "# warm up\n",
    "cu_zarr_afdist(z_root)\n",
    "gc, t1, t2 = cu_zarr_afdist(z_root)\n",
    "\n",
    "print(gc)\n",
    "print(\"Summarisation Time (ms) = {:.3f}\".format(t1))\n",
    "print(\"Host to GPU Transfer Time (ms) = {:.3f}\".format(t2*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068afd0c-d877-4326-ad76-ea528490d542",
   "metadata": {
    "tags": []
   },
   "source": [
    "As the results show, letting the GPU do the allocation of computation, hugely reduces the execution time.\n",
    "\n",
    "To deal with larger datasets without overwhelming GPU memory, the chunking approach can be used, so that each chunk is sufficiently large to fully utilise the GPU. These chunk sizes can then be used within the Zarr storage, to maximise efficiency. Where latency needs to be minimised, chunk sizes can be reduced and separate streams created, so that as one batch is being moved into GPU memory, the other is processing the previous batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d02d1-29ab-46ca-a8a7-101a4eea6f1b",
   "metadata": {},
   "source": [
    "## Zarr Decompression Using GPU on call_AD data\n",
    "\n",
    "The same, call_AD, data has been saved into several different versions of a Zarr array - each using a different algorithm. \n",
    "The GPU arrays are being loaded into GPU memory as a cupy array whereas the standard CPU Zarr data is being loaded into a numpy array.\n",
    "The data (int16) is the same size as the call_genotype (96514, 2504, 2) but has a 3rd dimension size of 7 rather than 2\n",
    "\n",
    "N.B. GPUDirect Storage has not been used here, which provides direct memory transfers from NVMe to GOU memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba4707f-ba7c-491a-941c-635efb903797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96514, 2504, 7)\n",
      "GPU Bitcomp compress time = 1.372 (s)\n",
      "GPU Snappy compress time = 0.996 (s)\n",
      "GPU LZ4 compress time = 0.937 (s)\n",
      "GPU Cascaded compress time = 0.703 (s)\n",
      "GPU Gdeflate compress time = 0.955 (s)\n"
     ]
    }
   ],
   "source": [
    "import kvikio\n",
    "import cupy as cp\n",
    "import zarr\n",
    "import kvikio.zarr\n",
    "import time\n",
    "\n",
    "def compress_nv(data, comp, root):\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    z1 = zarr.array(cp_call_ad,\n",
    "        chunks=(20000,cp_call_ad.shape[1],2),\n",
    "        store=root,\n",
    "        meta_array=cp.empty(()),\n",
    "        compressor=comp,\n",
    "        overwrite=True)\n",
    "    \n",
    "    return time.time()-t\n",
    "\n",
    "z = zarr.open(dpath + \"real_data/data/WGS/chr22.zarr\", mode='r')\n",
    "\n",
    "cp_call_ad=cp.array(z['call_AD'][:])\n",
    "print(cp_call_ad.shape)\n",
    "\n",
    "root = kvikio.zarr.GDSStore(dpath + \"real_data/data/WGS/chr22_cuda_gt_bitcomp.zarr\", normalize_keys=True)\n",
    "t = compress_nv(cp_call_ad, kvikio.zarr.Bitcomp(), root)\n",
    "print(\"GPU Bitcomp compress time = {:.3f} (s)\".format(t))\n",
    "\n",
    "root = kvikio.zarr.GDSStore(dpath + \"real_data/data/WGS/chr22_cuda_gt_snappy.zarr\", normalize_keys=True)\n",
    "t = compress_nv(cp_call_ad, kvikio.zarr.Snappy(),root)\n",
    "print(\"GPU Snappy compress time = {:.3f} (s)\".format(t))\n",
    "\n",
    "root = kvikio.zarr.GDSStore(dpath + \"real_data/data/WGS/chr22_cuda_gt_lz4.zarr\", normalize_keys=True)\n",
    "t = compress_nv(cp_call_ad, kvikio.zarr.LZ4(),root)\n",
    "print(\"GPU LZ4 compress time = {:.3f} (s)\".format(t))\n",
    "\n",
    "root = kvikio.zarr.GDSStore(dpath + \"real_data/data/WGS/chr22_cuda_gt_casc.zarr\", normalize_keys=True)\n",
    "t = compress_nv(cp_call_ad, kvikio.zarr.Cascaded(),root)\n",
    "print(\"GPU Cascaded compress time = {:.3f} (s)\".format(t))\n",
    "\n",
    "root = kvikio.zarr.GDSStore(dpath + \"real_data/data/WGS/chr22_cuda_gt_gdeflate.zarr\", normalize_keys=True)\n",
    "t = compress_nv(cp_call_ad, kvikio.zarr.Gdeflate(),root)\n",
    "print(\"GPU Gdeflate compress time = {:.3f} (s)\".format(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be65a7c-d69f-48db-843f-34200514a3c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Data has been compressed using the GPU (no comparison is made with CPU compression time here, but GPU saving and compression can be very quick). There are several different compression types, with each suited to a different data type (see [here](https://developer.nvidia.com/nvcomp))\n",
    "\n",
    "Now the same compressed Zarr files can be loaded and the time measured and compared to the CPU Zarr version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25a22f-21c2-46cd-93b2-e144d10874d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU snappy deflate time =  0.606 (s)\n",
      "GPU bitcomp deflate time =  0.626 (s)\n",
      "GPU cascade deflate time =  0.574 (s)\n",
      "GPU gdeflate deflate time =  0.967 (s)\n",
      "GPU lz4 deflate time =  0.567 (s)\n",
      "CPU zarr time =  7.560 (s)\n"
     ]
    }
   ],
   "source": [
    "import kvikio\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import zarr\n",
    "import kvikio.zarr\n",
    "import time\n",
    "\n",
    "# To prevent initialisation skewing the result, load both a CPU and GPU zarr library\n",
    "z = kvikio.zarr.open_cupy_array(store=\"real_data/data/WGS/chr22_cuda_ad_snappy.zarr\", mode='r')\n",
    "a=z[:]\n",
    "del(a)\n",
    "z1 = zarr.open_group(store=\"real_data/data/WGS/chr22.zarr\", mode='r')\n",
    "arr2 = cp.array(z1[\"call_genotype\"][:])\n",
    "del(arr2)\n",
    "# Data is deleted and then the actual measurements start\n",
    "\n",
    "t = time.time()\n",
    "z = kvikio.zarr.open_cupy_array(store=\"real_data/data/WGS/chr22_cuda_ad_bitcomp.zarr\", mode='r')\n",
    "a=z[:]\n",
    "print(\"GPU Bitcomp deflate time = {:.3f} (s)\".format(time.time()-t))\n",
    "del(a)\n",
    "\n",
    "t = time.time()\n",
    "z = kvikio.zarr.open_cupy_array(store=\"real_data/data/WGS/chr22_cuda_ad_snappy.zarr\", mode='r')\n",
    "a=z[:]\n",
    "print(\"GPU Snappy deflate time = {:.3f} (s)\".format(time.time()-t))\n",
    "del(a)\n",
    "\n",
    "t = time.time()\n",
    "z = kvikio.zarr.open_cupy_array(store=\"real_data/data/WGS/chr22_cuda_ad_lz4.zarr\", mode='r')\n",
    "a=z[:]\n",
    "print(\"GPU LZ4 deflate time = {:.3f} (s)\".format(time.time()-t))\n",
    "del(a)\n",
    "\n",
    "t = time.time()    \n",
    "z = kvikio.zarr.open_cupy_array(store=\"real_data/data/WGS/chr22_cuda_ad_casc.zarr\", mode='r')\n",
    "a=z[:]\n",
    "print(\"GPU Cascaded deflate time = {:.3f} (s)\".format(time.time()-t))\n",
    "del(a)\n",
    "\n",
    "t = time.time()\n",
    "z = kvikio.zarr.open_cupy_array(store=\"real_data/data/WGS/chr22_cuda_ad_gdeflate.zarr\", mode='r')\n",
    "a=z[:]\n",
    "print(\"GPU Gdeflate deflate time = {:.3f} (s)\".format(time.time()-t))\n",
    "del(a)\n",
    "\n",
    "t = time.time()\n",
    "z1 = zarr.open_group(store=\"real_data/data/WGS/chr22.zarr\", mode='r')\n",
    "arr2 = z1[\"call_AD\"][:]\n",
    "\n",
    "print(\"CPU zarr time = \", time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f4c82d-ad48-46f0-a33f-273f6b6f739e",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix\n",
    "\n",
    "For some kernels, it can be worth using the Shared Memory feature of a GPU. Shared Memory (only visible to threads within the same block) is faster than global memory (visibile to all threads), but is limited.\n",
    "In this case it makes little or no difference, but for other tasks in which memory is being accessed repeatedly, it can offer noticeable speed ups. For reference, below is an implementation of the kernel that uses Shared Memory.\n",
    "For more information in cupy kernels, see [here](https://docs.cupy.dev/en/stable/user_guide/kernel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04540b8b-6ad8-4f62-86f9-2e75a52125cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit.rawkernel()\n",
    "def cu_count_genotypes_chunk(G, hom_ref, hom_alt, het, ref_count,vs_size):\n",
    "    # Get the index of the current thread within a 2D block\n",
    "    thread_idx_x = jit.threadIdx.x # The x dimension is along the variant axis\n",
    "    thread_idx_y = jit.threadIdx.y # The y dimension is along the sample axis\n",
    "    \n",
    "    # The combination of block size, block index and thead index provide the  variant and sample indexes\n",
    "    variant_idx = jit.blockIdx.x * jit.blockDim.x + thread_idx_x \n",
    "    sample_idx = jit.blockIdx.y * jit.blockDim.y + thread_idx_y\n",
    "    sm_offset = (thread_idx_x*threads_per_block+thread_idx_y)*4\n",
    "    \n",
    "    # Shared memory is shared between threads within the same block\n",
    "    # It is limited in size but faster than global memory so can be used to cache data\n",
    "    shmem = jit.shared_memory(cp.int32,threads_per_block*threads_per_block*4)\n",
    "    \n",
    "    # Initialise the shared memory for the cached counts\n",
    "    shmem[sm_offset + 0]=0 # used for hom_ref\n",
    "    shmem[sm_offset + 1]=0 # used for hom_alt\n",
    "    shmem[sm_offset + 2]=0 # used for het\n",
    "    shmem[sm_offset + 3]=0 # used for ref_count\n",
    "    \n",
    "    # because we round the thread block size up to the problem size, check the bounds\n",
    "    if variant_idx<vs_size[0]:\n",
    "        if sample_idx < vs_size[1]:\n",
    "            a = G[variant_idx, sample_idx, 0]\n",
    "            b = G[variant_idx, sample_idx, 1]\n",
    "            if a == b:\n",
    "                if a == 0:\n",
    "                    shmem[sm_offset] += 1\n",
    "                else:\n",
    "                    shmem[sm_offset + 1] += 1\n",
    "            else:\n",
    "                shmem[sm_offset + 2] += 1\n",
    "            shmem[sm_offset + 3] += (a==0)\n",
    "            shmem[sm_offset + 3] += (b==0)\n",
    "    \n",
    "    # Now add the final shared memory cache vakues to the\n",
    "    # global memory arrays that were passed in\n",
    "    # using atomic adds to avoid data races\n",
    "    atomic_add(hom_ref,variant_idx,shmem[sm_offset])\n",
    "    atomic_add(hom_alt,variant_idx, shmem[sm_offset+1])\n",
    "    atomic_add(het,variant_idx, shmem[sm_offset+2])\n",
    "    atomic_add(ref_count,variant_idx, shmem[sm_offset+3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
