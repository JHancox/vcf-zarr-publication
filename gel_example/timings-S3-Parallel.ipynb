{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e88d29-57c0-47f5-b14a-d5e39a3275b9",
   "metadata": {},
   "source": [
    "## AFDist on S3 - parallel timing\n",
    "\n",
    "### Instance type is c5.24xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a97d60-cc07-4b23-a845-be147af7afc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (1.35.93)\n",
      "Requirement already satisfied: matplotlib in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: aioboto3 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (7.0.0)\n",
      "Requirement already satisfied: aiobotocore in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (2.17.0)\n",
      "Requirement already satisfied: numcodecs in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (0.14.1)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.93 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from boto3) (1.35.93)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from boto3) (0.10.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.2 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from aiobotocore) (3.9.5)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from aiobotocore) (0.12.0)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from aiobotocore) (6.1.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from aiobotocore) (1.26.19)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from aiobotocore) (1.17.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore) (1.5.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore) (1.18.0)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.0 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.9.2->aiobotocore) (3.10)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/libraries/envs/cloudos/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.9.2->aiobotocore) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 matplotlib aioboto3 aiobotocore numcodecs\n",
    "import boto3\n",
    "import aioboto3\n",
    "from botocore.config import Config\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defaabbd-4471-4b9c-a45f-0c96a2703b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3', config=Config(max_pool_connections=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c12823b2-a534-4007-9972-3609ccdf0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket=\"lifebit-user-data-1f2bfdf2-1d99-488c-9b87-246c62b66ea7\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066df16-f5c8-4afe-b985-9f82c62d1c96",
   "metadata": {},
   "source": [
    "## Firstly make a list of the chunks that are read by the serial process in the other \"Comparison\" notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6826df-3462-43b3-aee2-7e28ae893612",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index, end_index = 13889170, 14451810\n",
    "start_chunk, end_chunk = start_index//10000, end_index//10000\n",
    "keys = [(v,s) for v in range(start_chunk, end_chunk) for s in range(79)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b54cf6-5a99-4bcd-bba7-59b5e19635f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numcodecs.blosc import Blosc\n",
    "from numcodecs import blosc\n",
    "blosc.init()\n",
    "blosc.set_nthreads(1)\n",
    "#Need to call this twice, not sure why!\n",
    "blosc.set_nthreads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74395b-4d8b-4f7e-ae6e-a040ca18b6df",
   "metadata": {},
   "source": [
    "## Numba afdist code - note that this can be run in parallel writing to the same numpy output arrays as long as the chunks are independent, which they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02870a41-8309-4629-94f2-c2a794532c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba \n",
    "from numba import types\n",
    "import numpy as np\n",
    "\n",
    "@numba.jit(\n",
    "    (\n",
    "    types.int64,  # offset\n",
    "    types.Array(types.int8, 3, 'C', readonly=True),  # G (blosc buffer)\n",
    "    types.Array(types.int32, 1, 'C'),  # hom_ref\n",
    "    types.Array(types.int32, 1, 'C'),  # hom_alt\n",
    "    types.Array(types.int32, 1, 'C'),  # het\n",
    "    types.Array(types.int32, 1, 'C')   # ref_count\n",
    "), nopython=True, nogil=True\n",
    ")\n",
    "def count_genotypes_chunk_subset(\n",
    "    offset, G, hom_ref, hom_alt, het, ref_count):\n",
    "    #NB Assuming diploids and no missing data!\n",
    "    index = offset\n",
    "    for j in range(G.shape[0]):\n",
    "            for k in range(G.shape[1]):\n",
    "                    a = G[j, k, 0]\n",
    "                    b = G[j, k, 1]\n",
    "                    if a == b:\n",
    "                        if a == 0:\n",
    "                            hom_ref[index] += 1\n",
    "                        else:\n",
    "                            hom_alt[index] += 1\n",
    "                    else:\n",
    "                        het[index] += 1\n",
    "                    ref_count[index] += (a == 0) + (b == 0)\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c60c0ff-469d-42e9-b490-110e6f12e256",
   "metadata": {},
   "source": [
    "## Code for fetching, decoding and running AFDist in parallel processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c27edc15-aaaa-408f-b957-679534a14abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aioboto3\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import statistics\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import numpy as np\n",
    "\n",
    "# Total num of variants to allocate result arrays\n",
    "NUM_VAR=59880903\n",
    "\n",
    "@dataclass\n",
    "class BatchStats:\n",
    "    process_id: int\n",
    "    start_time: float\n",
    "    total_bytes: int\n",
    "    data_bytes: int\n",
    "    files_completed: int\n",
    "    duration: float = None\n",
    "    \n",
    "class S3BulkFetcher:\n",
    "    def __init__(self, bucket: str, keys: List[str], num_processes: int = None, batch_size: int = 500):\n",
    "        self.bucket = bucket\n",
    "        self.keys = keys\n",
    "        self.num_processes = num_processes or mp.cpu_count()\n",
    "        self.batch_size = batch_size\n",
    "        self.work_queue = mp.Queue()\n",
    "        self.stats_queue = mp.Queue()\n",
    "        \n",
    "    def prepare_batches(self):\n",
    "        batches = [\n",
    "            self.keys[i:i+self.batch_size] \n",
    "            for i in range(0, len(self.keys), self.batch_size)\n",
    "        ]\n",
    "        for batch in batches:\n",
    "            self.work_queue.put(batch)\n",
    "            \n",
    "        for _ in range(self.num_processes):\n",
    "            self.work_queue.put(None)\n",
    "    \n",
    "    async def process_batch(self, batch: List[str], session, decoder) -> BatchStats:\n",
    "        stats = BatchStats(\n",
    "            process_id=os.getpid(),\n",
    "            start_time=time.time(),\n",
    "            total_bytes=0,\n",
    "            data_bytes=0,\n",
    "            files_completed=0\n",
    "        )\n",
    "\n",
    "        hom_ref = np.zeros(NUM_VAR, dtype=\"int32\")\n",
    "        hom_alt = np.zeros(NUM_VAR, dtype=\"int32\")\n",
    "        het = np.zeros(NUM_VAR, dtype=\"int32\")\n",
    "        ref_count = np.zeros(NUM_VAR, dtype=\"int32\")\n",
    "        \n",
    "        async with session.client('s3') as s3:\n",
    "            async def fetch_single(key: str):\n",
    "                v, s = key\n",
    "                key = f\"GEL-256534/call_genotype/{v}/{s}/0\"\n",
    "                response = await s3.get_object(Bucket=self.bucket, Key=key)\n",
    "                raw_data = await response['Body'].read()\n",
    "                return raw_data, v\n",
    "            loop = asyncio.get_running_loop()\n",
    "\n",
    "\n",
    "            \n",
    "            decompression_tasks = []\n",
    "            for raw_data_v_chunk in asyncio.as_completed(\n",
    "                [fetch_single(key) for key in batch]\n",
    "            ):\n",
    "                raw_data, v_chunk = await raw_data_v_chunk\n",
    "                G = np.reshape(np.frombuffer(decoder.decode(raw_data), dtype=\"int8\"), (10000,1000,2))\n",
    "                count_genotypes_chunk_subset(v_chunk*10000,G, hom_ref, hom_alt, het, ref_count)\n",
    "                stats.total_bytes += len(raw_data)\n",
    "                stats.data_bytes += G.nbytes\n",
    "                stats.files_completed += 1                \n",
    "                    \n",
    "        stats.duration = time.time() - stats.start_time\n",
    "        return stats\n",
    "    \n",
    "    async def worker_loop(self):\n",
    "        session = aioboto3.Session()\n",
    "        decoder = Blosc.from_config({'cname': 'zstd', 'clevel': 7, 'shuffle': 2, 'blocksize': 0})\n",
    "        while True:\n",
    "            batch = self.work_queue.get()\n",
    "            if batch is None:  # sentinel value\n",
    "                break\n",
    "                \n",
    "            stats = await self.process_batch(batch, session, decoder)\n",
    "            self.stats_queue.put(stats)\n",
    "\n",
    "\n",
    "    def worker_process(self):\n",
    "        asyncio.run(self.worker_loop())\n",
    "\n",
    "    def print_final_stats(self, all_stats: List[BatchStats], wall_time: float):\n",
    "        total_bytes = sum(s.total_bytes for s in all_stats)\n",
    "        total_data = sum(s.data_bytes for s in all_stats)\n",
    "        total_files = sum(s.files_completed for s in all_stats)\n",
    "        \n",
    "        process_stats = defaultdict(lambda: {'bytes': 0, 'data':0, 'files': 0, 'batches': 0})\n",
    "        for stat in all_stats:\n",
    "            process_stats[stat.process_id]['bytes'] += stat.total_bytes\n",
    "            process_stats[stat.process_id]['data'] += stat.data_bytes\n",
    "            process_stats[stat.process_id]['files'] += stat.files_completed\n",
    "            process_stats[stat.process_id]['batches'] += 1\n",
    "\n",
    "        batch_times = [s.duration for s in all_stats]\n",
    "        \n",
    "        print(\"\\n=== S3 Bulk Download Statistics ===\")\n",
    "        print(f\"\\nOverall Performance:\")\n",
    "        print(f\"Total transfer: {total_bytes / 1000 / 1000:.2f} MB in {total_files} files\")\n",
    "        print(f\"Total data: {total_data / 1000 / 1000:.2f} MB in {total_files} chnnks\")\n",
    "        print(f\"Duration: {wall_time:.2f} seconds\")\n",
    "        print(f\"Throughput: {(total_bytes / 1000 / 1000) / wall_time:.2f} MB/s, \"\n",
    "             f\"{total_files / wall_time:.1f} files/s\")\n",
    "        print(f\"Data Throughput: {(total_data / 1000 / 1000) / wall_time:.2f} MB/s, \"\n",
    "             f\"{total_files / wall_time:.1f} chunks/s\")\n",
    "        \n",
    "        print(\"\\nBatch Statistics:\")\n",
    "        print(f\"Batch times - min: {min(batch_times):.2f}s, \"\n",
    "             f\"max: {max(batch_times):.2f}s, \"\n",
    "             f\"avg: {statistics.mean(batch_times):.2f}s\")\n",
    "        \n",
    "        print(\"\\nPer-Process Performance:\")\n",
    "        for pid, stats in process_stats.items():\n",
    "           print(f\"Process {pid}:\")\n",
    "           print(f\"  Throughput: {(stats['bytes'] / 1000 / 1000) / wall_time:.2f} MB/s, \"\n",
    "                 f\"{stats['files'] / wall_time:.1f} files/s\")\n",
    "           print(f\"  Data Throughput: {(stats['data'] / 1000 / 1000) / wall_time:.2f} MB/s, \"\n",
    "                 f\"{stats['files'] / wall_time:.1f} chunks/s\")\n",
    "           print(f\"  Processed: {stats['files']} files in {stats['batches']} batches\")\n",
    "\n",
    "    def run(self):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare work batches\n",
    "        self.prepare_batches()\n",
    "        \n",
    "        # Start worker processes\n",
    "        processes = []\n",
    "        for _ in range(self.num_processes):\n",
    "            p = mp.Process(target=self.worker_process)\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "            \n",
    "        # Collect all stats\n",
    "        all_stats = []\n",
    "        stats_to_collect = len([b for b in self.keys[::self.batch_size]])\n",
    "        \n",
    "        while len(all_stats) < stats_to_collect:\n",
    "            stats = self.stats_queue.get()\n",
    "            all_stats.append(stats)\n",
    "            \n",
    "            # Optional: Print progress\n",
    "            #if len(all_stats) % 10 == 0:\n",
    "            #    print(f\"Progress: {len(all_stats)}/{stats_to_collect} batches complete\")\n",
    "        \n",
    "        # Wait for all processes to complete\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "            \n",
    "        wall_time = time.time() - start_time\n",
    "        \n",
    "        # Print final statistics\n",
    "        self.print_final_stats(all_stats, wall_time)\n",
    "        \n",
    "        return all_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99085c90-11d1-42e9-bbe3-26277b5387fc",
   "metadata": {},
   "source": [
    "## Run and time the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25c0c7e9-7b21-40a5-b07a-db49bd697fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== S3 Bulk Download Statistics ===\n",
      "\n",
      "Overall Performance:\n",
      "Total transfer: 180.80 MB in 4503 files\n",
      "Total data: 90060.00 MB in 4503 chnnks\n",
      "Duration: 4.14 seconds\n",
      "Throughput: 43.63 MB/s, 1086.6 files/s\n",
      "Data Throughput: 21732.70 MB/s, 1086.6 chunks/s\n",
      "\n",
      "Batch Statistics:\n",
      "Batch times - min: 0.36s, max: 4.01s, avg: 3.72s\n",
      "\n",
      "Per-Process Performance:\n",
      "Process 1604:\n",
      "  Throughput: 0.02 MB/s, 0.7 files/s\n",
      "  Data Throughput: 14.48 MB/s, 0.7 chunks/s\n",
      "  Processed: 3 files in 1 batches\n",
      "Process 1558:\n",
      "  Throughput: 0.89 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1553:\n",
      "  Throughput: 0.64 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1561:\n",
      "  Throughput: 0.65 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1571:\n",
      "  Throughput: 0.85 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1581:\n",
      "  Throughput: 0.87 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1552:\n",
      "  Throughput: 0.69 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1583:\n",
      "  Throughput: 0.89 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1570:\n",
      "  Throughput: 0.92 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1566:\n",
      "  Throughput: 0.83 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1585:\n",
      "  Throughput: 0.80 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1565:\n",
      "  Throughput: 0.85 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1588:\n",
      "  Throughput: 0.78 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1587:\n",
      "  Throughput: 1.09 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1567:\n",
      "  Throughput: 1.12 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1591:\n",
      "  Throughput: 1.12 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1551:\n",
      "  Throughput: 1.10 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1573:\n",
      "  Throughput: 0.83 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1560:\n",
      "  Throughput: 0.77 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1579:\n",
      "  Throughput: 1.00 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1597:\n",
      "  Throughput: 1.18 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1563:\n",
      "  Throughput: 0.88 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1578:\n",
      "  Throughput: 0.94 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1589:\n",
      "  Throughput: 1.06 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1554:\n",
      "  Throughput: 0.97 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1559:\n",
      "  Throughput: 1.21 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1555:\n",
      "  Throughput: 1.08 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1564:\n",
      "  Throughput: 0.82 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1590:\n",
      "  Throughput: 1.02 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1598:\n",
      "  Throughput: 1.05 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1556:\n",
      "  Throughput: 0.74 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1602:\n",
      "  Throughput: 1.01 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1568:\n",
      "  Throughput: 1.10 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1576:\n",
      "  Throughput: 0.98 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1574:\n",
      "  Throughput: 1.08 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1562:\n",
      "  Throughput: 0.75 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1586:\n",
      "  Throughput: 1.31 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1594:\n",
      "  Throughput: 1.38 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1557:\n",
      "  Throughput: 0.67 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1575:\n",
      "  Throughput: 1.39 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1577:\n",
      "  Throughput: 1.00 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1582:\n",
      "  Throughput: 0.89 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1580:\n",
      "  Throughput: 1.06 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1569:\n",
      "  Throughput: 0.86 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1572:\n",
      "  Throughput: 1.13 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "Process 1584:\n",
      "  Throughput: 1.36 MB/s, 24.1 files/s\n",
      "  Data Throughput: 482.63 MB/s, 24.1 chunks/s\n",
      "  Processed: 100 files in 1 batches\n",
      "4.1475303173065186\n",
      "CPU times: user 84.2 ms, sys: 849 ms, total: 933 ms\n",
      "Wall time: 4.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fetcher = S3BulkFetcher(\n",
    "    bucket=bucket,\n",
    "    keys=keys,\n",
    "    num_processes=128,\n",
    "    batch_size=100,\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "stats = fetcher.run()\n",
    "duration = time.time() - start_time\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa04b0ba-8f72-4555-8f46-8138d6712796",
   "metadata": {},
   "source": [
    "### This gives a a total wall time of 4.15s for the run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (libraries-cloudos)",
   "language": "python",
   "name": "conda-env-libraries-cloudos-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
