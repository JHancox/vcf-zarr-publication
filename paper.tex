%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Please note that whilst this template provides a
% preview of the typeset manuscript for submission, it
% will not necessarily be the final publication layout.
%
% letterpaper/a4paper: US/UK paper size toggle
% num-refs/alpha-refs: numeric/author-year citation and bibliography toggle

%\documentclass[letterpaper]{oup-contemporary}
\documentclass[a4paper,num-refs]{oup-contemporary}

%%% Journal toggle; only specific options recognised.
%%% (Only "gigascience" and "general" are implemented now. Support for other journals is planned.)
\journal{gigascience}

\usepackage{graphicx}
\usepackage{siunitx}

% Additional packages for data model figure
\usepackage{forest}
\usepackage{tikz}
\usetikzlibrary{quotes,arrows.meta,3d}

% Local definitions to systematise tool naming
\newcommand{\sgapi}[1]{\texttt{#1}}
\newcommand{\toolname}[1]{\texttt{#1}}
\newcommand{\sgkit}{\texttt{sgkit}}


%%% Flushend: You can add this package to automatically balance the final page, but if things go awry (e.g. section contents appearing out-of-order or entire blocks or paragraphs are coloured), remove it!
% \usepackage{flushend}

% Macros etc for data model figure
\input{diagrams/data-model-preamble.tex}

% See https://github.com/sgkit-dev/vcf-zarr-publication/issues/87
% for discussion
\title{Analysis-ready VCF at Biobank scale using Zarr}

%%% Use the \authfn to add symbols for additional footnotes, if any. 
% 1 is reserved for correspondence emails; then continuing with 2 etc for contributions.
% First author
\author[1,\authfn{1}]{Eric Czech} % https://orcid.org/0000-0002-4254-4255
\author[2,3\authfn{1}]{Timothy R. Millar} % https://orcid.org/0000-0002-5142-8811
\author[4,\authfn{1}]{Tom White} 

% Middle
\author[5]{Ben Jeffery} % https://orcid.org/0000-0002-1982-6801
\author[6]{Alistair Miles} % https://orcid.org/0000-0001-9018-4680
\author[7]{Sam Tallman} % https://orcid.org/0000-0001-7183-6276
\author[1]{Rafal Wojdyla} % https://orcid.org/0009-0005-0735-7090
\author[8]{Shadi Zabad} % https://orcid.org/0000-0002-8003-9284
 
% Senior 
\author[1,\authfn{2}]{Jeff Hammerbacher} % https://orcid.org/0000-0001-6596-8563
\author[5,\authfn{2},\authfn{3}]{Jerome Kelleher} % https://orcid.org/0000-0002-7894-5253

\affil[1]{Related Sciences}
\affil[2]{The New Zealand Institute for Plant \& Food Research Ltd, Lincoln,
New Zealand}
\affil[3]{Department of Biochemistry, School of Biomedical Sciences, University of Otago, Dunedin, New Zealand}
\affil[4]{Tom's Institute}
\affil[5]{Big Data Institute, Li Ka Shing Centre for Health Information and Discovery, 
University of Oxford, UK}
\affil[6]{Wellcome Sanger Institute}
\affil[7]{Genomics England}
\affil[8]{School of Computer Science, McGill University, Montreal, QC, Canada}

%%% Author Notes
\authnote{\authfn{1}Joint first author.}
\authnote{\authfn{2}Joint senior author.}
\authnote{\authfn{3}jerome.kelleher@bdi.ox.ac.uk}

%%% Paper category
\papercat{Paper}

%%% "Short" author for running page header
\runningauthor{Czech et al.}

%%% Should only be set by an editor
\jvolume{00}
\jnumber{0}
\jyear{2024}

\begin{document}

\begin{frontmatter}
\maketitle

% The Abstract (250 words maximum) should be structured to
% include the following details:
% \textbf{Background}, the context and purpose of
% the study;
% \textbf{Results}, the main findings;
% \textbf{Conclusions}, brief
% summary and potential implications. Please minimize the use of abbreviations
% and do not cite references in the abstract.
% The Abstract (250 words maximum) should be structured to
% include the following details:
% \textbf{Background}, the context and purpose of % the study;
% \textbf{Results}, the main findings;
% \textbf{Conclusions}, brief summary and potential implications.

%% NOTE: this is much too long currently, but keeping for now so we
% can see which stuff migrates to the intro
\begin{abstract}
\textbf{Background:}
Variant Call Format (VCF) is the standard file format for interchanging
genetic variation data and associated quality control metrics.
It provides a well-defined data model and is central to a large ecosystem
of tools interoperating through this format.
The standard row-based encoding of the VCF data model (either as text
or packed binary) emphasises efficient retrieval of all data for a given
variant, but accessing subsets of the data within rows is inefficient.
Biobank-scale datasets currently available 
consist of hundreds of thousands of whole genomes with tens of terabytes
of compressed VCF per chromosome.
Row-based data storage is fundamentally unsuited to such datasets
and these large VCFs represent a major computational bottleneck.

\textbf{Results:}
We present the VCF Zarr specification, a columnar encoding of the 
VCF data model using Zarr which makes retrieving subsets of the 
data much more efficient. Zarr is a cloud-native format for storing 
multi-dimensional numerical data, widely used in scientific computing.
We show how this format is competitive with specialised methods for 
storing genotype data in terms of compression ratios, and also performs
well on compressing the high-entropy quality-control fields that 
tend to dominate. We show that performing calculations on the genotype
matrix is much more efficient than standard
approaches based on VCF, and is comparable with state-of-the-art methods
using compiled languages. We demonstrate 
the VCF Zarr format (and the vcf2zarr conversion utility) 
on the Genomics England dataset of XX,XXX,XXX samples, showing
an overall reduction in storage of XX, and a reduction in processing 
time of common queries of up to XX fold.

\textbf{Conclusions:}
Large row-encoded VCF files are a major bottleneck for current research, and 
processing these files incurs a substantial economic cost.
The VCF Zarr specification, building on widely-used, open-source technologies
has the potential to greatly reduce these costs,
and may enable a diverse ecosystem of next-generation tools for analysing 
genetic variation data.
\end{abstract}

\begin{keywords}
Variant Call Format; Zarr; Analysis ready data.
\end{keywords}
\end{frontmatter}

%%% Key points will be printed at top of second page
\begin{keypoints*}
\begin{itemize}
\item VCF is widely supported, and the underlying data model entrenched 
in Bioinformatics pipelines.
\item The standard row-wise encoding as text (or binary) is inherently
inefficient for large-scale data processing.
\item The Zarr format provides an efficient solution, by encoding fields
in the VCF separately in chunk-compressed binary format.
\end{itemize}
\end{keypoints*}

\section{Background}
Variant Call Format (VCF) is the standard format for interchanging genetic
variation data~\citep{danecek2011variant}, storing information about 
DNA sequence polymorphisms among a set of samples with associated 
quality control metrics and metadata. 
Originally defined specifically as a text file,
it has been refined and standardised~\citep{rehm2021ga4gh} and the 
underlying data-model is now deeply embedded in bioinformatics practice.
Dataset sizes have grown explosively since the introduction of 
VCF as part of 1000 Genomes project~\citep{10002015global},
with Biobank-scale projects such as 
Genomics England~\cite{turnbull2018100},
UK Biobank~\citep{bycroft2018genome,backman2021exome,halldorsson2022sequences,uk2023whole},
and the All of Us research program~\citep{all2024genomic}
collecting genome sequence data for hundreds of thousands of humans.
While there are many advantages to encoding data in text files,
VCF is simply not fit for purpose as part of computational workflows 
at this scale. 
Methods that require efficient access to data
either require VCF data to be converted into 
PLINK~\cite{purcell2007plink,chang2015second} 
or BGEN~\citep{band2018bgen} 
format~\citep[e.g.][]{yang2011gcta,mbatchou2021computationally,loh2015efficient}
or use 
custom binary formats~\citep[e.g.][]{
% Uses custom "bref3" format,
% https://faculty.washington.edu/browning/beagle/bref3.24May18.pdf
browning2018one, 
% .samples Zarr format
kelleher2019inferring,
% Has a "xcftools" package, but it still looks pretty experimental
hofmeister2023accurate} 
that support the required access patterns.
While PLINK and BGEN support efficient access to genotype data, neither
can accommodate the full flexibility of the VCF data model and conversion
is lossy. PLINK's approach of storing the genotype matrix in uncompressed
packed-binary format also leads to large files, particularly for recent
whole-genome sequence datasets. For example, at two bits 
per genotype, the full matrix for the GraphTyper SNP dataset in the UKB
WGS data~\citep{uk2023whole} is 116 TiB.
% 1,037,556,156 SNPs x  490,640 samples
% humanize.naturalsize(1_037_556_156 * 490_640 / 4, binary=True)
% '115.7 TiB'

The main solution that has emerged to these problems of 
computation on genetic variation data at scale is Hail~\cite{ganna2016ultra,hail2024}.
Building on components from the Hadoop ecosytem~\citep{white2012hadoop},
Hail distributes computations transparently over a cluster, enabling
interactive analysis of very large datasets. It has been successfully used 
in many studies, and instrumental in generating datasets such as 
gnomadAD~\cite{karczewski2020mutational,chen2024genomic}.
While Hail is powerful software, it is not a suitable platform for 
developing inference or performing complex analysis. It is complex,
vertically integrated software, with an undocumented file format
[say this better]. 
Similarly, commerical solutions that have emerged to facilitate
the analysis of large-scale genetic variation data are either
based on proprietary~[Terra? BigQuery?]
or single-vendor technologies~\cite[e.g.][]{tiledb2024}.

VCF has enabled a vibrant ecosystem of interoperating
utilities~\cite{danecek2021twelve,garrison2022spectrum}, but these tools
are fundamentally scale-limited by the format. 
The most intrinsically limiting aspect of VCF's design is its 
row-wise layout of data within a file, which means that (for example)
data for a particular sample cannot be efficiently retrieved.
The file-oriented paradigm is also unsuited to the realities 
of modern datasets, which are too large to download and 
often required to stay in-situ by data-access agreements.
Large files are stored in cloud environments, where the 
file systems that are required by file-oriented tools
are expensively emulated on the basic building blocks
of object storage. 
These multiple layers of inefficiences around processing
VCF data at scale in the cloud mean that it is 
time-consuming and expensive, effectively making 
% Dunno if we want to get into this, but it's true
analysis of these datasets the preserve of a well-funded elite.

Here, we decouple the VCF data model from its row-oriented
file definition, and show how the corresponding data model can be 
efficiently stored and analysed in a cloud-native manner.
We do this by translating VCF data into Zarr format,
a popular method of storing compressed N-dimensional arrays
used to store petabyte-scale datasets across the sciences [citations].
We present the VCF Zarr specification which formalises this 
mapping, and and the \texttt{vcf2zarr} 
utility which reliably converts large-scale VCFs into Zarr 
format. We show that VCF Zarr is much more compact than 
VCF (and BCF), and it competitive with state-of-the-art
VCF compression tools. Moreover, we show that Zarr's 
storage of data in an analysis-ready 
format~\cite{abernathey2021cloud} greatly facilitates computation,
with various benchmarks being substantially faster than
\texttt{bcftools} based pipelines, and again competetive
with state-of-the-art methods. Finally, we show the 
utility of VCF Zarr on the Genomics England aggV2 dataset,
demonstrating that common queries can be performed orders
of magnitude more quickly based on simple Python scripts.


\section{Results}

\subsection{Storing genetic variation data}
Although VCF is the standard format for exchanging genetic variation
data, its limitations both in terms of compression 
and query/compute performance are well 
known~\citep[e.g.][]{kelleher2013processing,layer2016efficient,li2016bgt},
and many methods 
have been suggested to improve on these properties.
Most approaches balance compression with
performance on particular types of queries, 
typically using a command line interface
and outputting VCF text~\citep{
layer2016efficient, %GQT
li2016bgt, % BGT
tatwawadi2016gtrac, % GTRAC
danek2018gtc, % GTC
lin2020sparse, % SpVCF
lan2020genozip,lan2021genozip, %genozip
lefaive2021sparse, % SAVVY
wertenbroek2022xsi,% XSI
zhang2023gbc}. %GBC
Several specialised algorithms for compressing 
the genotype matrix (i.e., just the genotype calls without additional
VCF information) have been proposed
\citep{deorowicz2013genome, %TGC
deorowicz2019gtshark, %GTShark
deorowicz2021vcfshark, % VCFShark
dehaas2024genotype} %  GRG
most notably the Positional
Burrows--Wheeler Transform (PBWT)~\citep{durbin2014efficient}.
See~\citep{mcvean2019linkage} for a review of the techniques
employed in genetic data compression.

VCF is a text-based format in which each line
describes observations
and metadata for a single variant (usually a position on the genome
where variation occurs in the samples)~\citep{danecek2011variant}. 
VCF is therefore a \emph{row-wise} format, i.e., where all of the 
data for a record is stored continguously. BCF is similarly 
row-wise, as are the majority of proposed alternative storage formats.
Row-wise storage makes retrieving all information
for a given record straightforward and efficient, and 
allows data to be easily streamed through Unix pipelines (see Discussion).

Row-wise storage works well when records are relatively small, 
or when we typically want to analyse each record in its entirity.
When we want to analyse only a subset of a record,
row-wise storage can be inefficient because we will usually need to
retrieve more information than required from storage. In the case 
of VCF (and BCF) where records are not of a fixed size and 
are almost always compressed in blocks, accessing any information
for all rows means retrieving and decompressing \emph{all} 
information from all rows.
The alternative to row-wise storage is \emph{columnar} storage:
instead of grouping together all the fields for a record,
we group together all the records for a given field.
Columnar storage has become standard [very quick summary of columnar
storage with some refs].
Although columnar methods have been successfully used in alignment 
% TODO check
storage~\citep{bonfield2014scramble,bonfield2022cram},
the only existing method using columnar 
storage for genetic variation data is 
SeqArray~\citep{zheng2017seqarray,zheng2012high} to our knowledge.

\begin{figure}
\resizebox{225pt}{!}{\input{diagrams/data-model.tex}}
\caption{Columnar storage of VCF data in chunked, compressed form using Zarr.
[TODO more detail, including brief overview of how data maps from 
VCF to VCF Zarr in the examples]
\label{fig-data-model}}
\end{figure}

We propose the VCF Zarr specification which maps the 
VCF data model into a columnar layout using the
widely-used Zarr format (Fig~\ref{fig-data-model}). 
Briefly, Zarr is a format 
% FIXME two different uses of $n$ here in a paragraph
for storing $n$-dimensional arrays of a fixed datatype, stored as 
compressed chunks. In the VCF Zarr specification, 
each field in a VCF is mapped to an array, which is compressed 
and stored individually allowing for efficient retrieval. 
In particular, call-level data is stored as $m \times n$ arrays
(for $m$ sites and $n$ samples), allowing for efficient 
retrieval of subsets of those fields along both the 
variants and samples axis.
Note that this approach to storing higher dimensional data
sets Zarr apart from more general columnar formats 
such as Parquet [TODO more details and citations?].
See the Methods for more detail on Zarr and the VCF Zarr
specification.

\begin{figure}
\begin{center}
\includegraphics[]{figures/data-scaling}
\end{center}
\caption{Compression performance on simulated genotypes.
Comparison of total stored bytes for VCF data produced 
by subsets of a large simulation of French-Canadians.
Sizes for $10^6$ samples are shown on the right. Sizes 
for Savvy (21.25GiB) and Zarr (22.06GiB) are very similar.
\label{fig-data-storage}}
\end{figure}

Remarkably, the simple approach of compressing
two dimensional chunks with the Zstandard 
compressor~\citep{collet2021rfc} and the bit-shuffle
filter from Blosc~\cite{alted2010modern} produces 
compression levels competitive with specialised methods
on the genotype matrix. In Fig~\ref{fig-data-storage}
we show compression performance on up to a million
samples subset a large and highly realistic 
simulation of French-Canadians~\cite{anderson2023on}
(see Methods for details).
Note that while simulations cannot capture 
all the subtleties of real data, the allele frequency
and population structure patterns in this dataset 
have been shown to closely follow 
observations~\cite{anderson2023on} and so provides 
a reasonable data point when comparing such methods.
% Could note here that this might represent a best 
% case scenario for specialised methods, but maybe 
% not worth bothering with.

[Short note here on why we choose these tools to
benchmark against, and why there's no point in 
a major benchmarkathon. Some recent papers 
have good benchmarks which the interested 
reader can cross-reference]

Although Zarr is optimised for cloud settings, it also
works very well in the classical (shared) file-system setting
and we therefore focus on benchmarks in this setting
to allow us to compare with existing tools.
Those seeking benchmarks of Zarr in cloud deployments are 
directed to~\citep{durbin2020task,moore2021ome,gowan2022using} [ANY OTHERS?]


\subsection{Calculating with the genotype matrix}
Storing genetic variation data compactly is important, but it is also
important that we can analyse the data efficiently. Bioinformatics 
workflows tend to emphasise text files and command line utilities 
that consume and produce text~\citep[e.g.][]{buffalo2015bioinformatics}. 
Thus, many tools that compress VCF data provide a command line 
utility with a query language to restrict the records
examined, perform some pre-specified calculations and finally 
output some text, typically VCF or tab/comma separated values
[TODO check these citations]~\citep{
layer2016efficient, %GQT
li2016bgt, % BGT
tatwawadi2016gtrac, % GTRAC
danek2018gtc, % GTC
lin2020sparse, % SpVCF
lan2020genozip,lan2021genozip, %genozip
lefaive2021sparse, % SAVVY
wertenbroek2022xsi,% XSI
zhang2023gbc}. %GBC
These pre-defined calculations are by necessity limited in scope, however,
and the volumes of text involved in Biobank scale datasets
make the classical approach of custom
analyses via Unix utilities in pipelines prohibitively slow. Thus, 
methods have begun to provide Application Programming Interfaces
(APIs), providing efficient access to genotype and other VCF data
[TODO check these citations]~\cite[e.g.][]{kelleher2013processing,lefaive2021sparse,
wertenbroek2022xsi,zhang2023gbc}. By providing programmatic access,
the data can be retrieved from storage, decoded and then analysed
in the same memory space, avoiding the need for copying
the decoded data and inter-process communication. [TODO some refs
on zero-copy semantics and things like Arrow?]

To demonstrate the accessibility of genotype data and efficiency with 
which calculations can be performed under the different formats,
we use the \texttt{bcftools +af-dist} plugin as an example of such a whole-matrix
computation. 
% The details of the \texttt{af-dist} operation are not important:
% as an example of a whole-matrix operation. 
We chose this particular operation for several
reasons. First, it is a simple but non-trival calculation that requires 
examining every element in the genotype matrix.
Secondly, it produces a small volume of output (a table of
deviations from Hardy-Weinberg expectations in ten allele frequency
bins) and therefore the time spent outputting results is negligible.
Finally, it has an efficient implementation written using the 
\texttt{htslib} C API~\citep{bonfield2021htslib}, distributed as 
a \texttt{bcftools} plugin. 

\begin{figure}
\includegraphics{figures/whole-matrix-compute}
\caption{Whole-matrix compute performance with increasing sample size.
Total CPU time required to run \texttt{bcftools +af-dist}
and equivalent operations in a single thread for various tools.
Elapsed time is also reported (dotted line). Run-time for genozip
at $10^6$ samples was extrapolated by fitting an exponential.
See Methods for full details.
\label{fig-whole-matrix-compute}}
\end{figure}

Figure~\ref{fig-whole-matrix-compute} shows the results of 
running \texttt{bcftools +af-dist} and equivalent operations 
on the data of Figure~\ref{fig-data-storage}. There is a large
difference in the time required (note the log-log scale). 
The slowest approach uses Genozip. Because genozip does not
provide an API and only outputs VCF text, the best approach available 
is to pipe its output into \texttt{bcftools +af-dist}. Thus,
not only must we decode the data from the genozip format, we must 
then parse the large volumes of VCF text and then finally do the 
actual calculation. Running \texttt{bcftools +af-dist} directly
on the gzipped VCF text incurs essentially the same cost, but
the decompression cost is lower and without the interprocess 
communication overhead. Using a BCF file is somewhat faster,
because the packed binary format can be read into the internal 
data structures used by the \texttt{htslib} API more efficiently.

The data shown in Figure~\ref{fig-whole-matrix-compute} for Zarr and Savvy
is based on custom programs written using their respective APIs
to implement the \texttt{af-dist} operation. The Zarr program uses
the Zarr-Python package to iterate over the decoded chunks of the 
genotype matrix and classifies genotypes within a chunk using a 14 line Python
function, accelerated using the Numba JIT compliler~\cite{lam2015numba}.
The allele frequencies and genotype counts are then analysed to produce 
the final counts within the allele frequency bins with 9 lines of 
Python using NumPy~\cite{harris2020array} functions. Remarkably, this 
short and simple Python program is substantially faster than the 
equivalent compiled C using \texttt{htslib} APIs on BCF (6.9 hours
vs 20.6 hours for 1 million samples). 
% num_samples  num_sites      tool  user_time  sys_time     wall_time  total_time
%     1000000    7254858     savvy    8371.07      4.16   8377.718136  2.326453
%     1000000    7254858  bcftools   73939.32     46.09  74023.083678  20.551503
%     1000000    7254858      zarr   24709.64     29.32  24750.704171  6.871933
The fastest method is the 
C++ program written using the Savvy API. This would largely seem
to be due to Savvy's excellent genotype decoding performance
% zarr 1.2 GiB
% savvy 6.6 GiB
% zarr_nshf 3.9 GiB
(up to 6.6GiB/s vs 1.2GiB/s for Zarr on this dataset;
Figure~\ref{fig-whole-matrix-decode}).
% sequence_length  num_samples  num_sites       tool       size
% 40         48129895      1000000    7254858       zarr  22.071687
% 35         48129895      1000000    7254858        bcf  51.749294
% 36         48129895      1000000    7254858    genozip  10.691384
% 37         48129895      1000000    7254858        sav  21.249436
% 38         48129895      1000000    7254858        tsk   1.802636
% 39         48129895      1000000    7254858        vcf  81.375831
% 41         48129895      1000000    7254858  zarr_nshf  29.897540
Turning off the BitShuffle filter for the Zarr dataset,
however, leads to a substantial increase in decoding speed
(3.9GiB/s) at the cost of a roughly 25\% increase in storage
space (29.9GiB up from 22.1GiB for 1 million samples; data not
shown). Given the relatively small contribution of genotypes to the
overall storage of real datasets (see the Genomics England example)
and the frequency that they are likely to be accessed, this
would seem like a good tradeoff in most cases.
This ability to easily tune compression performance
and decoding speed on a field-by-field basis is a major strong
point of Zarr. The \texttt{vcf2zarr} utility also provides
functionality to aid with such storage schema tuning.


\subsection{Subsetting the genotype matrix}
As datasets grow ever larger, the ability to efficiently access subsets 
of the data becomes more and more important. VCF/BCF achieve efficient 
access to the data for genomic ranges 
by compressing blocks of adjacent records using \texttt{bgzip},
and storing secondary indexes alongside the original 
files with a conventional suffix~\citep{li2011tabix}. 
Thus, for a given range query we 
decompress only the necessary blocks and can quickly access
the required records. 
Methods like Savvy and XSI largely follow the same principles [TODO check and 
be more concrete]. The row-wise nature of these methods, however, mean 
that we cannot efficiently subset \emph{by sample}. In the extreme
case, if we want to access only the genotypes for a single sample
we must still read in and decompress the entire dataset.

\begin{figure}
\includegraphics{figures/subset-matrix-compute}
\caption{Compute performance on subsets of the matrix.
Total CPU time required to run the af-dist calculation for
a subset of 10000 variants $\times$ 10 samples from the middle of the matrix
for the data in Figure~\ref{fig-data-storage}.
The \texttt{genozip} and \texttt{bcftools} pipelines involve
multiple commands required to correctly calculate the AF INFO field
required by \texttt{bcftools +af-dist}. See the Methods for full details
on the steps performed.
\label{fig-subset-matrix-compute}}
\end{figure}

We illustrate the cost of row-wise encoding in
Figure~\ref{fig-subset-matrix-compute}, where we run the af-dist calculation
on a small fixed-size subset of the genotype matrices of
Figure~\ref{fig-data-storage}. The two-dimensional chunking of Zarr
means that this small subset of the overall matrix can be efficiently
extracted, and therefore the execution time depends very weakly on 
the overall dataset size, with the overall computation requiring around
1 second for 1 million samples. Because of their 
row-wise encoding for all the other methods CPU time
scales with the number of samples.
Figure~\ref{fig-subset-matrix-compute-supplemental} shows performance
for the same operation when selecting half of the samples in the 
dataset.

\begin{figure}
\includegraphics{figures/column-extract}
\caption{Time to extract the genome position and write to a text file.
Total CPU time required to extract the POS field for BCF,
sav and Zar formats
for the data in Figure~\ref{fig-data-storage}.
For the BCF file we used \texttt{bcftools query -f"\%POS$\backslash$n"}.
For sav, we used the Savvy C++ API to extract just the position, 
and output text using the \texttt{std::cout} stream. For Zarr, we read 
\texttt{variant\_position} into a NumPy array, and then wrote to
a text file using the Pandas
\texttt{write\_csv} method. Writing
text dominates in the case of Zarr: populating the NumPy array 
in memory takes less than a second in all cases. Genozip does
not offer an option to extract individual columns, and so would require
the output to be piped into \texttt{bcftools query} or a
Unix utility such as \texttt{cut}.
\label{fig-column-extract}}
\end{figure}

\subsection{Extracting fields}
We have focused on the genotype matrix up to this point, contrasting
Zarr with existing row-wise methods. The genotype matrix, however,
is only one aspect of VCF data.
Real-world VCFs encapsulate much more than just the genotype
matrix, and can contain large numbers of additional fields. Efficient
extraction of the data from these fields is a key use-case
and an area where columnar storage has clear benefits.
Figure~\ref{fig-column-extract} shows the time required to extract 
the genomic position of each variant in the simulated benchmark 
dataset, which we can use as an indicative example of a per-variant 
query. Although Savvy is many times faster than \texttt{bcftools query}
here, the row-wise storage strategy that they share means that 
the entire dataset must be read into memory and 
decompressed to extract just one field from each record. Zarr's
columnar storage excels at these tasks: we only read and decompress
the information required.

\subsection{Case study: Genomics England 100,000 genomes}
In this section we demonstrate the utility of VCF Zarr on a large human dataset
and the scalability of the \texttt{vcf2zarr} conversion utility.
Genomics England’s multi-sample VCF dataset (aggV2) is an 
aggregate of 78,195 gVCFs from rare disease and cancer participants 
recruited as part of the 100,000 Genomes Project~\cite{turnbull2018100}. 
The dataset comprises approximately 722 million annotated single-nucleotide 
variants (SNVs) and small indels split into 1,371 roughly equal chunks and 
totalling 165.3 TiB of VCF data after \texttt{bgzip} compression. 
The dataset is used for a variety of research purposes, ranging from 
genome-wide association studies (GWAS)~\cite{kousathanas2022whole} and 
imputation~\cite{shi2023genomics} to 
simple queries involving single gene 
regions~\cite{leggatt2023genotype,lam2023repeat}.

As described in the Methods, conversion to Zarr using 
\texttt{vcf2zarr} is a two-step 
process to limit memory usage and ensure reliability. We 
first converted the 28 VCF files (3.51 TiB) for chromosome 20
into the intermediate columnar format (ICF). This task was 
split into 3899 partitions, and distributed using the Genomics England
HPC cluster. It required about 50 days of CPU, with an elapsed
wall-clock time of about 2 hours. The ICF representation uses a total
of XTiB over XX files and YY directories. We then encoded this intermediate
columnar format to Zarr using 8 cores and XX GB of RAM in about 12 hours.
This produced a dataset with 43 arrays, consuming a 
total of Y GiB of storage over X directories and 
Y files. This is about a 4.8X reduction over the original VCF. 
The top fields in terms 
of storage are detailed in Table~\ref{tab-genomics-england-data}.

\begin{table}
\caption{Summary for a selection of the largest VCF Zarr columns produced for 
Genomics England aggV2 VCFs on chromosome 20 using \texttt{vcf2zarr}
default settings. Each field is stored independently 
as a Zarr array with the given type (sufficient to represent all values in the
data). We show the total storage consumed (reported via \texttt{du}) in 
power-of-two units, and the compression ratio achieved on that array.
We also show the percentage of the overall storage that each array consumes.
\label{tab-genomics-england-data}}
\begin{tabular}{llS[table-format=3.1]S[table-format=3.2]S[table-format=3.2]}
\toprule
{Field} & {type} & {storage} & {compress} & {\%total} \\
\midrule
call\_AD &  int16 & 179.7G & 26 & 24.0\\
call\_GQ &  int16 & 171.8G & 13 & 23.0 \\
call\_DP &  int16 & 141.8G & 16 & 18.9 \\
call\_DPF& int16  & 115.1G & 20 & 15.3\\
call\_FT &  string & 58.5G & 160 & 7.8 \\
call\_PL &  int16 & 51.4G & 140 & 6.9 \\
call\_GQX &  int16 & 12.1G & 190 & 1.6 \\
call\_genotype & int8 & 6.1G & 380 & 0.8 \\
call\_genotype\_mask & bool & 3.7G  & 630 & 0.5\\
call\_genotype\_phased & bool & 692.5M  & 1700 & 0.1 \\
call\_PS  & int8  & 102.2M & 12000 & 0.05 \\
variant\_quality & float32 & 22.6M & 2.7 & <0.01 \\
variant\_allele & string & 21.5M & 11 \\
variant\_position & int32 & 12.7M & 4.7 \\
variant\_ABratio & float32 & 9.6M & 6.3 \\
variant\_AN & int32 & 9.6M & 6.3 \\
variant\_filter & bool & 1.9M & 490 \\
\bottomrule
\end{tabular}
\end{table}

% array([23.956     , 22.90533333, 18.90133333, 15.34533333,  7.8       ,
%         6.85333333,  1.61066667,  0.81733333,  0.488     ])

% /call_AD                     int16    179.67 GiB  4.53 TiB       26 125768  37.74 MiB     1.46 MiB            (15912293, 78195, 2)  (10000, 1000, 2)  
% /call_GQ                     int16    171.79 GiB  2.26 TiB       13 125768  18.87 MiB     1.4 MiB             (15912293, 78195)     (10000, 1000) 
% /call_DP                     int16    141.76 GiB  2.26 TiB       16 125768  18.87 MiB     1.15 MiB            (15912293, 78195)     (10000, 1000)
% /call_DPF                    int16    115.09 GiB  2.26 TiB       20 125768  18.87 MiB     959.56 KiB          (15912293, 78195)     (10000, 1000)
% /call_FT                     object   58.5 GiB    9.05 TiB      160 125768  75.48 MiB     487.71 KiB          (15912293, 78195)     (10000, 1000)
% /call_PL                     int16    51.4 GiB    6.79 TiB      140 125768  56.61 MiB     428.51 KiB          (15912293, 78195, 3)  (10000, 1000, 3)
% /call_GQX                    int16    12.08 GiB   2.26 TiB      190 125768  18.87 MiB     100.73 KiB          (15912293, 78195)     (10000, 1000) 
% /call_genotype               int8     6.13 GiB    2.26 TiB      380 125768  18.87 MiB     51.11 KiB           (15912293, 78195, 2)  (10000, 1000, 2)  
% /call_genotype_mask          bool     3.66 GiB    2.26 TiB      630 125768  18.87 MiB     30.53 KiB           (15912293, 78195, 2)  (10000, 1000, 2)  
% /call_genotype_phased        bool     692.51 MiB  1.13 TiB     1700 125768  9.43 MiB      5.64 KiB            (15912293, 78195)     (10000, 1000) 
% /call_PS                     int8     102.2 MiB   1.13 TiB    12000 125768  9.43 MiB      852 bytes           (15912293, 78195)     (10000, 1000) 
% /call_ADR                    int8     102.2 MiB   1.13 TiB    12000 125768  9.43 MiB      852 bytes           (15912293, 78195)     (10000, 1000)
% /call_ADF                    int8     102.2 MiB   1.13 TiB    12000 125768  9.43 MiB      852 bytes           (15912293, 78195)     (10000, 1000)
% /variant_OLD_MULTIALLELIC    object   31.63 MiB   121.4 MiB       3.8 1592  78.09 KiB     20.34 KiB           (15912293,)           (10000,) 
% /variant_quality             float32  22.6 MiB    60.7 MiB        2.7 1592  39.04 KiB     14.53 KiB           (15912293,)           (10000,) 
% /variant_allele              object   21.52 MiB   242.8 MiB      11 1592  156.17 KiB    13.84 KiB           (15912293, 2)         (10000, 2) 
% /variant_position            int32    12.83 MiB   60.7 MiB        4.7 1592  39.04 KiB     8.25 KiB            (15912293,)           (10000,) 
% /variant_ABratio             float32  9.62 MiB    60.7 MiB        6.3 1592  39.04 KiB     6.18 KiB            (15912293,)           (10000,) 
% /variant_AN                  int32    9.6 MiB     60.7 MiB        6.3 1592  39.04 KiB     6.18 KiB            (15912293,)           (10000,) 
% /variant_AC                  int32    9.22 MiB    60.7 MiB        6.6 1592  39.04 KiB     5.93 KiB            (15912293,)           (10000,) 
% /variant_AC_Het              int32    8.98 MiB    60.7 MiB        6.8 1592  39.04 KiB     5.78 KiB            (15912293,)           (10000,) 

% Approx total = 179.67 + 171.79 + 141.76 + 115.09 + 58.5 + 51.4 + 12.08 + 6.13 + 3.66 
% 740, round up to 750
% [179.67 , 171.79 , 141.76 , 115.09 , 58.5 , 51.4 , 12.08 , 6.13 , 3.66 ]
% >>> import numpy as np
% >>> a = np.array([179.67 , 171.79 , 141.76 , 115.09 , 58.5 , 51.4 , 12.08 ,
% 6.13 , 3.66 ]  )
% >>> a
% array([179.67, 171.79, 141.76, 115.09,  58.5 ,  51.4 ,  12.08,   6.13,
%          3.66])
% >>> a / 750 * 100
% array([23.956     , 22.90533333, 18.90133333, 15.34533333,  7.8       ,
%         6.85333333,  1.61066667,  0.81733333,  0.488     ])

Table~\ref{tab-genomics-england-data} shows that the dataset storage
size is dominated by a few columns with the top four
(call\_AD, call\_GQ, call\_DP and call\_DPF) accounting for over 
80\% of the total. These fields are much less compressible
than genotype data (which uses $<1\%$ of the total space here)
because of their inherent noisiness~\citep{lin2020sparse}. By 
default the \texttt{bio2zarr} conversion is lossless, and integers
of the narrowest type required to represent all of the data are
used. In this case, this has resulted in 16 bit integers being used 
for these top four fields. Examining the distribution of the depth
fields, (call\_AD, call\_DP, call\_DPF), we can 
we can see that a tiny fraction of the values stored
are greater than 127 (call\_AD: 0.1\%, call\_DP: xx\%, call\_DPF: x\%; see 
Fig SX for histograms).
One might then reasonably truncate these fields to 8 bit integers.
After converting to 8 bit, call\_AD is reduced by 
x\% (XX GiB) and call\_DP reduced by x\% (XX GiB)
and call\_DPF by x\% (XX GiB). 
Similarly, the vast majority of call\_GQ values (xx\%) are less than 
128 (corresponding to a conditional probability that the genotype call is 
wrong of $10^{-128}$ [TODO CHECK THIS]), and can also reasonably
be truncated. Here, this results in a reduction of x\% (XX GiB).
Combined, truncating these top four fields to 8 bits reduces the 
overall storage of the dataset to XXGiB, corresponding to a 
X-fold compression of the VCF.

% and can reasonably be truncated (a 
% much information would be lost if we used 8 bit
% Substantial
% reductions in space can be achieved, however, if we examine the 
% distributions of these fields and truncate some outliers.
% Figure~SX shows the histogram of the XX call\_AD values, and we can
% see that < 0.0X\% of the values are $>127$. Truncating and 
% re-encoding the call\_AD array as an int8 results in a total storage of 
% XX. A substantial advantage of Zarr is that such per-field optimisations
% can be made after conversion, without affecting the rest of the dataset.
% As discussed in Methods, judicious use of additional filters
% and lossy quantizing can result substantial space savings.

Calculating the histogram of per call values at this 
scale is not a trivial task.
Taking advantage of the rich software ecosystem growing around Zarr
(see Discussion) we used xarray~\citep{hoyer2017xarray}
and Dask~\citep{rocklin2015dask} to transparently distribute the 
computations over a cluster of X workers. 
[ADD DETAILS]
The elapsed time was X minutes, and requires X lines of Python in a 
Jupyter notebook~\citep{kluyver2016jupyter}. 

One of the key uses of the aggV2 dataset is to enable simple queries 
of specific regions. For example, in [A PAPER] they [DID SOMETHING].
We replicated this by running the appropriate bcftools query
on the VCF chunk (note that the user is responsible for finding 
the correct chunk file here, limiting the utility of genomic 
indexes). This took XXX hours.
The same query implemented with the Zarr-python 
API required XXX seconds to produce identical output.

\section{Discussion}
The introduction of VCF as part of the work of the 1000 Genomes
project~\cite{10002015global} has transformed a fragmented 
landscape of mutually incompatible per-program file formats
into an ecosystem of tools that largely
interoperate~\cite{garrison2022spectrum}. In this 
sense VCF has been a huge success, and the underlying data model
is now deeply embedded in bioinformatics practice. As we have 
illustrated here, the shortcomings of the VCF and BCF file
formats (and more generally \emph{any} row-wise encoding of the 
data model) in terms of computational accessibility are severe,
and there can be little doubt that these file formats 
are incurring a large economic cost and 
holding back progress. 
As we have illustrated in the Genomics England
example here, these shortcomings are most acutely felt in 
large-scale data human datasets but any setting in which we 
wish to extract columns of the data from a row-wise 
VCF format will be inefficient 
when datasets are larger than a few hundred megabytes.

As we have demonstrated, the columnar approach, based on the 
popular Zarr standard and widely used compression libraries, 
can support many different types of data access
patterm efficiently, and has excellent computational accessibility.
In this approach there are some significant departures from 
standard practise, which warrant some discussion here.

Perhaps the largest deviation in the approach that we advocate here 
from standard practise is to decouple our concept of a dataset from
a file. Each VCF is broken up into a set of Zarr arrays, each 
corresponding to a large number of chunks stored in a directory
hierarchy (Zarr also supports other forms of chunk storage 
including single files and key-value stores however; see Methods).
While there are some disadvantages to large numbers of small files,
we argue that the benefits greatly outweigh them, particularly 
in the large, centrally controlled dataset setting. 
Firstly, this approach is inherently parallelisable, and chunks across the 
dataset written and read in parallel without needing complex 
synchronisation and prior layout planning. 
Such parallel write access is very difficult 
in the single-file setting, and one of the key weaknesses 
of the HDF5 format~\citep{folk2011overview} [CITATION with 
parallel troubles from HDF5?]
Secondly, mapping chunks in arrays to individually addressed
keys maps well to large-scale cloud object storage. Indeed,
Zarr is specifically designed for such usage, and 
several petabyte scale datasets have been successfully deployed
and used [citations?].
We have focused here on the classical single-server and 
HPC cluster settings, but a major advantage of Zarr is 
its cloud readiness.
% Probably getting into the weeds too much.
% Some large cloud-hosted VCF datasets are currently stored in 
% filesystems that emulate POSIX semantics on top of such object 
% stores, at a substantial cost in performance. Directly hosting 
% these datasets in object stores would have substantial 
% benefits, both in terms of storage costs and performance.
Thirdly, breaking up fields into different directories (or 
cloud buckets) enables fine-grained access control to the 
dataset. For example, all users may be allowed access to
variant-level data, but only a subset permitted to access
call-level individual data. Such access controls can be 
straightforwardly applied using file or cloud-hosting 
methods on the relevant directores/buckets. Such concerns
are entirely decoupled from the file format.

% This is poorly explained
Along with the move away from the single-file-storage paradigm,
the approach we outline here also suggests a shift from 
the classical Unix pipeline-based processing approach 
(but see below for a discussion of how existing pipelines can
be supported). Datasets flowing through pipelines inherently 
implies creating muliple copies of the data, and when 
multiple terabytes of either unrelated or unchanged data 
get copied several times through these pipes large inefficiencies 
are incurred (see Fig~\ref{fig-whole-matrix-compute}). 
We may think of this strategy as ``copy-oriented'' workflows.
A more efficient way to structure such multilayered computations
is in a  ``mask-oriented'' way. For example, to implement some
complex multilayered call filtering strategy we could imagine sequentially
updating a given per-call mask. Masks are cheap (e.g., the 
standard genotype missingness mask in~\ref{tab-genomics-england-data}
uses less than 1\% of the overall storage).

Zarr provides pragmatic solutions to some of the more pressing 
problems facing the analysis of large-scale genetic variation
data, but it is not a solution to all problems. [large number
of alleles, dataset is basically static, ragged arrays,
strings ... any others? Zarr v3 on the way.]

Nonetheless, at least in the single-shared-copy model currently
used by most large-scale variation datasets, Zarr provides
an excellent basis for building a new generation of tools 
that compute \emph{directly} on the data, efficiently
accessing data by variant and by sample.
[Mention ``Cloud native data repositories''~\cite{abernathey2021cloud}]
The VCF Zarr specification and scalable vcf2zarr conversion utility
that we have provided here are the starting points of such 
an ecosystem, but much more would be needed to make a viable 
alternative for everyday bioinformatics workflows. However, a few key
pieces of infrastructure would provide a starting point, and would 
be sufficient to greatly improve researcher productivity on 
large, centrally managed datasets such as Genomics England.
Firstly, a \texttt{vcfzarrtools} command line utility which 
implements a subset of \texttt{bcftools} functionality (such as 
\texttt{view} and \texttt{query}) would immediately speed up 
limited region and field specific queries by orders of magnitude,
and would provide compatibility with existing workflows.
Developing such a utility would be a non-trivial but routine
engineering task.

The second key piece of infrastructure required at this large 
scale concerns distributed computation. Even with more efficient
columnar data representations, computations that required examing 
large volumes of per-call data must be distributed to complete 
in a reasonable amount of time. [Dask, Xarray, sgkit?]
Currently users are responsible for orchestrating such calculations
on clusters, and automation would be a significant boon
for reasearcher productivity. [Hail has demonstrated the value of this?]


% We suggest that the future of VCF is to explicitly define the underlying
% data model, and to decouple it from the current row-wise encodings 
% of the VCF text and BCF binary file formats. This will enable data 
% to flow more freely, and provide scope for much-needed innovation.

% \item A major strong point of Zarr is its simplicity. All it's doing
% is compressing n-dimensional chunks of numerical data and storing
% them using fixed address keys. This key-value approach means that storage
% is very flexible and implementation is straightforward. There
% are several implementations of the Zarr protocol, and while some of these
% are rudimentary, they could be improved or replaced with relatively
% little effort. Zarr has been successfully used to store petabytes
% of data across different sciences --- genetic variation data is largely
% arrays of numerical data, and not really that different ultimately.
% Zarr is cloud native.
% Zarr is an open specification, built on standard components (contrast
% with Hail).

% \item Vision. An ecosystem of tools that interoperate
% based on an open, cloud-based format where distribution is naturally
% managed by the stored chunks. Tools can be run on different datasets
% with minimal tuning, not by writing a full layer of orchestration.
% Tools read the data they need, not the entire dataset. Users read
% directly from a single canonical dataset, and do not maintain
% their own filtered copies. We suggest that the Zarr VCF approach
% we have provided would be a good starting point for designing
% such a compute platform.

% \end{itemize}

[Random fragment to include somewhere]
Row-wise storage of the genotype matrix makes accessing 
sample haplotypes (i.e. the columns),
required for many
applications~\cite[e.g.][]{durbin2014efficient,kelleher2019inferring} 
[TODO add phasing, imputation]
inefficient. The simple approach of storing the genotype matrix
in rectangular chunks allows efficient access both by row \emph{and}
column. Similarly, these rectangular chunks capture patterns
both of similarity between samples and linkage disequilibrium~\cite{mcvean2019linkage} 
between sites, such that conventional compression schemes 
perform very well~\ref{fig-data-storage}.


The Python data science ecosystem is increasingly popular, 
with packages such as NumPy~\citep{harris2020array},
SciPy~\citep{virtanen2020scipy},
Pandas~\citep{mckinney2010data},
Jupyter~\citep{kluyver2016jupyter}
enabling much of contemporary science.

BioNumpy~\citep{rand2022bionumpy}, 

% Although the Hadoop ecosystem has been hugely successful
% in many domains, it has had a limited impact in genomics.
% [List some things that have been tried for e.g variant calling
% , like ADAM~\citep{nothaft2015rethinking}].
% [Briefly mention two approaches using Hadoop clusters
% ~\citep{boufea2017managing,fan2020variant}, to not very
% exiting results].
% Distributed analytics for large-scale data is a mature
% techology, with many different approaches. Hadoop~\citep{white2012hadoop}
% is the original method, and remains the basis of the
% dominant ecosystem. HDFS. Spark [CITATION?] lets us
% do more complex analytics. Parquet [CITATION?]
% Distributed computation has been the reality in the analysis
% of genetic variation for many years,
% where data is most often provided as per-chromosome VCFs.
% This chunking provides a fairly straightforward way to parallelise
% computations, using HPC schedulers to split work per-file.
% When more fine-grained parallelism is required, users must
% provide exact genome coordinates of the genome regions
% as part of their submission scripts. Combining results
% is a manual operation.
% Workflow engines such as
% Snakemake~\cite{koster2012snakemake,molder2021sustainable}
% and [TODO mention WDL, Cromwell, etc?] make such analyses
% much easier, but there is still a great deal of room for
% mistakes.
% Per-chromosome VCFs are not practical for biobank scale
% projects, however, and variant call data tends to be split
% into many (still large) files.
% For example, the VCFs for 150K UKB WGS data~\cite{halldorsson2022sequences}
% are provided in 50kb chunks~\cite{browning2023statistical}, resulting in
% hundreds of files per chromosome.
% % This is horrible writing, but the point is important.
% While these files provide a natural
% unit for distributing work, the details of how they
% are split differ and essentially there is
% no interoperability among large-scale datasets because the VCFs are
% so cumbersome that a separate orchestration layer is required to
% access the data in parallel.

\section{Methods}

\subsection{Zarr and block-based compression}
% Zarr is a simple layer on top of best-in-class, modern components
In the interest of completeness it is useful to provide a high-level overview
of Zarr and the techologies that it depends upon. Zarr is a specialised format
for storing large-scale $n$-dimensional data (arrays). Arrays
are split into chunks, which are compressed and stored separately. Chunks are 
addressed by their indexes along the dimensions of the array, and the 
compressed data associated with this key. Chunks are
are often stored in individual files, but a wide array of different
stores are supported including cloud buckets and NoSQL databases;
in principle, Zarr can store data in any key-value store.
Metadata describing the array and its properties is then stored 
in JSON format along with the chunks. The simplicity and transparency
of this design has substantial advantages over other technologies
such as HDF5~\citep{folk2011overview} which are relatively complex and opaque.
This simplicity has led to numerous implementations of the Zarr specification
being developed, ranging from the mature Zarr-Python~\citep{zarrpython}
and TensorStore~\citep{tensorstore} implementations
to more experimental extensions to packages like
GDAL~\citep{gdal_zarr},
NetCDF~\citep{netcfd_c},
N5~\citep{n5zarr}
and xtensor~\citep{xtensor_zarr}
as well as  standalone libraries for JavaScript~\cite{zarrjs},
Julia~\cite{zarrjl}, Rust~\citep{zarrs}
and R~\cite{pizzarr}.

Zarr is flexible in allowing different compression codecs and 
pre-compression filters to be specified on a per-array basis
(see section XXX).
Two key technologies used by Zarr are the Blosc
% This seems like the best citation for Blosc? It is mentioned here
meta-compressor~\cite{alted2010modern}
and Zstandard compression algorithm~\citep{collet2021rfc}.
Blosc is a high-performance compressor optimised for numerical
data which uses ``blocking''~\citep{alted2010modern} to 
optimise CPU-cache access patterns, as well as highly optimised
bit and byte shuffle filters.  Remarkably, on highly 
compressible datasets, Blosc decompression can be faster 
than \texttt{memcpy}.
Blosc is written in C, with APIs for C, Python, Julia, Rust
and others.
Blosc is a ``meta-compressor'' because it provides different 
access to several different compression codecs. The 
Zstandard compressor~\citep{collet2021rfc} is of particular 
interest here as it achieves very high compression ratios
with fast decompression speeds [see fig]. 
% This may seem a bit off topic, but want to reassure reader that 
% this isn't some niche technology
Zstandard is also used in several recent VCF compression 
methods~\citep[e.g.][]{lefaive2021sparse,wertenbroek2022xsi}.
% LZ4 is also notable - any citations for this?

Scientific datasets are increasingly overwhelming the classical
model of downloading and analysing locally, and are migrating to 
centralised cloud repositories \citep{abernathey2021cloud,moore2021ome}.
The combination of Zarr's simple and cloud-friendly storage 
of data chunks with state-of-the-art compression methods has 
led to Zarr gaining significant traction in these settings.
Multiple petabyte-scale datasets are now stored using 
Zarr~\cite[e.g.][]{gowan2022using, % weather model data
fahnestock2023mappin, % ITS_LIVE dataset
cmip6_dataset}
or under active consideration for migration~\citep{durbin2020task,abernathey2021opening}.
The Open GeoSpatial consortium has formally recognised Zarr as a community
standard~\cite{ogc_zarr2_standard}
and has formed
a new GeoZarr Standards Working Group to establish a Zarr encoding for
geospatial data~\cite{ogc_geozarr_news}.

Zarr has recently been gaining popularity in biological applications.
The Open Microscopy Environment has developed OME-Zarr~\cite{moore2023ome} as one 
of its ``next generation'' cloud ready file formats~\citep{moore2021ome}.
OME-Zarr already has a rich suite of supporting
tools~\cite{moore2023ome,rzepka2023toward}. 
Zarr has also seen recent uptake 
in single-scell single-cell genomics~\citep{dhapola2022scarf,virshup2023scverse}
and multimodal spatial omics
data~\citep{marconato2024spatialdata,baker2023emobject}.





EUGENe~\citep{klie2023predictive} is a toolkit for 
for applying deep-learning methods to genomic sequence data,
integrating with PyTorch [citation?].




DivBrowse~\citep{konig2023divbrowse}. Works with VCF data using 
the scikit-allel \texttt{vcf\_to\_zarr} function, (produces 
a precursor to the VCF-Zarr specification here). It's a 
browser, using Zarr as the storage backend.

Zarr was originally developed as a means efficiently accessing VCF data,
 and has been in use within the MalariaGEN project for XX years. [
ALISTAIR - details and citations here!]

\subsection{The VCF Zarr specification}
The VCF Zarr specification is a direct mapping from the VCF data model
to a chunked binary array format using Zarr. VCF Zarr takes advantage
of Zarr's hierarchical structure by representing a VCF file as a top-level
Zarr group containing Zarr arrays. Each VCF field (fixed fields, INFO fields,
and FORMAT fields) is represented as a separate array in the Zarr hierarchy.
Some of the structures from the VCF header are also represented as arrays,
including contigs, filters, and samples. Making every item in VCF Zarr an
array means that the data can be accessed via a uniform API.

% Do we need to say that VCF Zarr is an evolution of the Zarr format in scikit-allel?

The specification defines the name, shape, dimension names, and data type
for each array in the Zarr store. These ``logical'' properties are mandated,
in contrast to ``physical'' Zarr array properties such as chunk sizes and
compression, which can be freely chosen by the implementation. This
separation makes it straightforward for tools and applications to consume
VCF Zarr data since the data has a well-defined structure, while allowing
implementations enough room to optimise chunk sizes and compression
according to the application's needs.

The specification defines a clear mapping of VCF field names (keys) to
array names, VCF Number to array shape, and VCF Type to array data type.
To take one example, consider the VCF AD genotype field defined by the
following VCF header: \texttt{\#\#FORMAT=<ID=AD,Number=A,Type=Integer,Description="Allele Depths">}

The FORMAT key \texttt{ID} maps to an array name of \texttt{call\_AD}
(FORMAT fields have a \texttt{call\_} prefix, while INFO fields have a
\texttt{variant\_} prefix; both are followed by the key name). Arrays
corresponding to FORMAT fields are 3-dimensional with shapes that look
like \texttt{(variants, samples, <Number>)} in general. In this case, the
Number A entry indicates that the field has one value per alternate allele,
which in VCF Zarr is represented as the \texttt{alt\_alleles} dimension name,
so the shape of this array is \texttt{(variants, samples, alt\_alleles)}.
The VCF Integer type can be represented as any Zarr integer type, and in
fact the specification doesn't mandate a particular integer width, so it
could be encoded as a 16 bit integer for example.
% Refer to GeL case study section?

VCF Zarr uses special sentinel values to encode missing values to indicate
that a value is not present, and fill values to pad variable length fields.
The latter is necessary since Zarr arrays have fixed size dimensions. The
specification also permits extra boolean arrays to accompany a field array
to redundantly encode the missing and fill status, which can be useful for
efficiently masking out values that shouldn't be processed, at the cost of
a small amount of extra storage.

The VCF Zarr specification can represent anything described by BCF
(which is somewhat more restrictive than BCF) except for two corner
cases related to the encoding of missing data. Firstly, VCF Zarr does
not distinguish between a field that is not present and one that 
is present but contains missing data. For example, a variant with an
INFO field \texttt{NS=.} is represented in the same way in VCF Zarr
as an INFO field with no \texttt{NS} key. Secondly, because of the use
of sentinel values to represent missing and fill values for integers
(-1 and -2, respectively), a field containing these original values
cannot be stored. In practice this doesn't seem to be much of 
an issue (we have not found a real VCF that contains negative 
integers). However, if -1 and -2 need to be stored, a float field
can be used without issues (since the float sentinel is a special NaN
value).

The VCF Zarr specification is general and can be mapped to 
file formats such as \toolname{plink}~\citep{purcell2007plink,chang2015second}
and BGEN~\citep{band2018bgen} with some minor extensions.

\subsection{vcf2zarr}
Converting VCF to Zarr format at Biobank scale is challenging. This main issue is 
around memory usage: although we can view each record in the VCF one-by-one,
we must buffer a full chunk (10,000 variants is reasonable choice) 
in the variants dimension for each of the arrays. 
For VCFs with many FORMAT fields and large numbers of samples this can
easily require tens of gigabytes of RAM per worker, which makes 
parallelism troublesome. Reading the VCF multiple times for different fields
is possible, but would be prohibitively slow for multi-terabyte VCFs.

The \texttt{vcf2zarr} utility solves this problem by first converting 
the VCF data (which can be split across many files) into an Intermediate
Columnar Format (ICF). The \texttt{vcf2zarr explode} command takes a set
of VCFs and stores the data in per-field chunks (by default, 64MiB).
In this way, the memory required is bounded by the size of single record 
BCF record in htslib (plus the size of a decoded bgzf record
[FIXME what is the correct terminology here?]), plus 64MiB for each 
field.  The Intermediate Columnar Format stores the data for each 
field spread across multiple partitions of the data (so that 
multiple parts of the VCF can be read in parallel), and then 
fixed size compressed chunks within those partitions. It also stores information
about the number of records in each partition and chunk of a given
field, so that the record at a particular index can be retrieved efficiently 
later. Summaries such as the minimum and maximum value 
of each field are also maintained, to aid choice of data types later.
A set of VCF files can be converted to intermediate columnar 
format in parallel on a single machine using a given number 
of worker processes using the \texttt{explode} command,
or can be distributed across a cluster using the 
\texttt{dexplode-init},
\texttt{dexplode-partition} and \texttt{dexplode-finalise} commands.

Once the VCF data has converted to the intermediate columnar format,
it can then be converted to Zarr using the \texttt{vcf2zarr encode}
command. Alhough there is a one-to-one mapping between the VCF
fields and the Zarr arrays, there is considerable flexibility 
in how the data is stored, for example in integer width
and compression and filter settings.
By default vcf2zarr chooses integer type sizes based 
on the range of values seen in the intermediate column format
and reasonable compressor defaults, but these can be
altered by generating a schema in JSON format and editing it.
Encoding a given column (for example, \texttt{call\_AD})
involves creating a buffer to hold a full variant-chunk of the 
array in question, and then sequentially filling this buffer with 
values read from the intermediate columnar format. Once the buffer 
is full, the chunks in the samples dimension are compressed
and written to file. [Something about parallelism and distributing]

Fields such as PL are currently problematic when there are large 
numbers of alleles. However, we plan on implementing the 
local alleles strategy~\citep{poterba2024scalable,danecek2021twelve}
which will likely become part of the VCF specification.

\section{Choosing default compressor settings}
To inform the choice of compression settings across different fields 
in VCF data, we analysed their effect on compression ratio on 
data from the 1000 Genomes project.

% [FILL IN WITH Shadi's analysis from here:
% https://github.com/sgkit-dev/bio2zarr/discussions/74
% Discussed in:
% https://github.com/sgkit-dev/vcf-zarr-publication/issues/26

\subsection{Benchmarks}
Full details of the benchmarking done in
Figure~\ref{fig-data-storage} and
Figure~\ref{fig-whole-matrix-compute}.
Quick notes:

\begin{itemize}
\item Dataset is based on subsets of French-Canadian sims. We simplify
for increasingly large subsets, keeping only variant sites. We then
convert to VCF and encode to BCF using
\texttt{tskit vcf | bcftools view -O b}.
\item We used a chunk size of XX for sgkit, after some experimentation.
This gave n files of around x MB each for the ``call\_genotypes`` array.
We used the defaults for savvy and genozip.
\item For the afdist CPU time we measure the sum of the total user and
system times required to execute the full command, as reported by GNU
time. Each tool was instructed to use one thread, where the options
were provided. For genozip, the time required to compute
\texttt{genocat file.genozip | bcftools +af-dist}. Because commands
in the pipeline execute concurrently on mutiple cores, the total CPU time is
greater than the elapsed time.
\item Where possible in pipelines we use uncompressed BCF
 output \texttt{-Ou} to make processing more efficient (skip printf
and parsing costs, which are substantial). Following advice
in~\citep{danecek2021twelve}.
\item We do not use BCF output in genozip because it doesn't support
it directly, only VCF (supports BCF by pipeing through bcftools).
\end{itemize}

\section{Availability of source code and requirements}

\section{Data availability}

\section{Declarations}

\subsection{List of abbreviations}
% If abbreviations are used in the text they should be defined in the text at
% first use, and a list of abbreviations should be provided in alphabetical
% order.

\begin{itemize}
    \item VCF: Variant Call Format;
    \item UKB: UK Biobank
    \item WGS: Whole Genome Sequence
\end{itemize}

\subsection{Competing Interests}
\subsection{Funding}

\subsection{Author's Contributions}

\section{Acknowledgements}


\bibliography{paper}

\clearpage
\renewcommand\thefigure{S\arabic{figure}}
\setcounter{figure}{0}
\renewcommand\thetable{S\arabic{table}}
\setcounter{table}{0}

\section*{Supplementary Material}

\begin{figure}
\includegraphics{figures/whole-matrix-decode}
\caption{Genotype decoding performance.
Total CPU time required to decode genotypes into memory using the zarr-python
and Savvy C++ APIs for the data in Figure~\ref{fig-data-storage}.
This corresponds to maximum rate of 1.2GiB/s for Zarr and 5.7GiB/s
for Savvy. [TODO add data for 1 million samples]
\label{fig-whole-matrix-decode}}
\end{figure}

\begin{figure}
\includegraphics{figures/subset-matrix-compute-supplemental}
\caption{Compute performance on a large subset of the genotype matrix.
Total CPU time required to run the af-dist calculation for
a subset of half of the samples and 10000 variants from the middle of the matrix
for the data in Figure~\ref{fig-data-storage}.
Genozip did not run for
$n > 10^4$ samples because it does not support a file to specify
sample IDs, and the command line was therefore too long for the shell
to execute. 
\label{fig-subset-matrix-compute-supplemental}}
\end{figure}

\end{document}
